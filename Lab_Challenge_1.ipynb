{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVlwEEkWlXEu"
      },
      "source": [
        "## **Lab Challenge 1:**\n",
        "\n",
        "#Author:\n",
        "\n",
        "Tathagat Saha (Mat no: 902046)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGl8O6kYqnvq"
      },
      "source": [
        "## LAB CHALLENGE: Neural Matrix Factorization \n",
        "So far we have developed GMF that applies a linear kernel to model the latent feature interactions and MLP that uses a non-linear kernel to learn the interaction function from data. \n",
        "\n",
        "How can we fuse GMF and MLP under the NCF framework, so that they can mutually reinforce each other to better model the complex user-item interactions?\n",
        "\n",
        "A straightforward solution is to let GMF and MLP share the same embedding layer and then combine the outputs of their interaction functions.\n",
        "\n",
        "To provide flexibility to the fused model we want to allow GMF and MLP to learn separate embeddings and combine two models by concatenating their last hidden layer. \n",
        "\n",
        "Formally\n",
        "\n",
        "$$\\phi^{GMF} = \\mathbf{p_u}^G\\odot \\mathbf{q_i}^G$$\n",
        "$$\\phi^{MLP} = a_L(\\mathbf{W}_L^T(a_{L-1}(...a_2 (\\mathbf{W}_2^T \\begin{bmatrix}\n",
        "\\mathbf{p_u} \\\\ \\mathbf{q_i}\n",
        "\\end{bmatrix} + \\mathbf{b}_2)...)) + \\mathbf{b}_L)$$\n",
        "\n",
        "$$ y_{ui} = \\sigma(\\mathbf{h}^T \\begin{bmatrix}\n",
        "\\ \\phi^{GMF} \\\\ \\phi^{MLP}\n",
        "\\end{bmatrix})$$\n",
        "\n",
        "that results in the following model architecture\n",
        "\n",
        "<center>  <img src=\"https://drive.google.com/uc?export=view&id=1gNLUpiQdbDPMdvfZYVs3lcou3cd4Favb\" width=\"550\" height=\"400\"> </center> \n",
        "\n",
        "\n",
        "TASK 1: implement the model as described \n",
        "TASK 2: compare the performance of such model with the GMF and MLP models using the metrics provided. \n",
        "TASK 3: tune the networks by plotting HR@10 and NDCG@10 with respect to the number of predictive factors [8, 16, 32, 64] for all the 3 algorithms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwK9BJ1bss0_"
      },
      "source": [
        "## Introduction to Recommender System\n",
        "\n",
        "Recommender systems are algorithms that mimic the psychology and personality of humans, in order to predict their needs and desires. \n",
        "\n",
        "There are many different types of techniques and implementations out there.\n",
        "\n",
        "- **Content-based methods** uses attributes of items to recommend to users new items similar to what the user has liked in the past (doesn't take into account the behaviour of other users);\n",
        "- **Collaborative Filtering methods** uses similarities between users and items simultaneously to determine recommendations;\n",
        "- **Hybrid methos** mix content-based and collaborative filtering approaches.\n",
        "\n",
        "Other approaches are also called **Knowlege-based methods** which uses explicit knowledge about users and items to build recommendations criteria with a rule-based approach.\n",
        "\n",
        "\n",
        "<center>  <img src=\"https://drive.google.com/uc?export=view&id=1Qaizz9YLvqgXg0blWFwN92IQSQPLTSoH\" width=\"950\" height=\"400\"> </center> \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9131NU_plgbG"
      },
      "source": [
        "### Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEu8PnwhNSn_"
      },
      "source": [
        "Here for completing the mentioned Lab Challenge we will be using the following packages of python, used for deep learning. PyTorch and NumPy both are similar but the advantage with PyTorch is that it can perform computation in CPU and GPU without any chage to the code.\n",
        "\n",
        "For working with array we are importing numpy PyTorch for deep learning tensor, Torch.nn for neural networks, Torch.optim for optimization problem Torch.util.data for loading data set and data loader.\n",
        "\n",
        "matplotlib is a library for mathematical plot or computation in NumPy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Yji9-7jslKRz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas_profiling as pdp\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arJu3Re7XCky",
        "outputId": "32d380f8-30c5-4944-fbdc-86499f331019"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f61783e6b10>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.random.seed(7)\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkX_xiKpNgrN"
      },
      "source": [
        "Select the device where the processing is meant to happen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "IkcVzJcY5TS_"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epmWgmA5w920"
      },
      "source": [
        "## Movielens Dataset\n",
        "\n",
        "\n",
        "For the sake of the development we are using \"u.data\" dataset provided to us. We mount google drive and link our model with this dataset as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8IHo3D4jlx8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "qcffYFMdxDcI"
      },
      "outputs": [],
      "source": [
        "num_sample_data = '100k'\n",
        "DATA_PATH = 'drive/MyDrive/Colab Notebooks/u.data' #change this with your directory \n",
        "MODEL_PATH = 'drive/MyDrive/movielens_{}/'.format(num_sample_data) #change this with your directory \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ43K6UT9UmJ"
      },
      "source": [
        "##Important Functions\n",
        "\n",
        "We now build some function to manage the data.\n",
        "\n",
        "We drop the exact value of rating (1,2,3,4,5) and instead convert it to an implicit scenario i.e. any positive interaction is given value of 1. All other interactions are given a value of zero, by default.\n",
        "\n",
        "Since we are training a classifier, we need both positive and negative samples. The records present in the dataset are counted as positive samples. We assume that all entries in the user-item interaction matrix are negative samples (a strong assumption, and easy to implement).\n",
        "\n",
        "We randomly sample 4 items that are not interacted by the user, for every item interacted by the user. This way, if a user has 20 positive interactions, he will have 80 negative interactions. These negative interactions cannot contain any positive interaction by the user, though they may not be all unique due to random sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE13kg0aydtu"
      },
      "source": [
        "We now define the class NCF_Data which will be used do read the data and create the train and test dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "hmlijxEoYXDj"
      },
      "outputs": [],
      "source": [
        "class Rating_Datset(Dataset):\n",
        "\tdef __init__(self, user_list, item_list, rating_list):\n",
        "\t\tsuper(Rating_Datset, self).__init__()\n",
        "\t\tself.user_list = user_list\n",
        "\t\tself.item_list = item_list\n",
        "\t\tself.rating_list = rating_list\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.user_list)\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\tuser = self.user_list[idx]\n",
        "\t\titem = self.item_list[idx]\n",
        "\t\trating = self.rating_list[idx]\n",
        "\t\t\n",
        "\t\treturn (\n",
        "\t\t\ttorch.tensor(user, dtype=torch.long),\n",
        "\t\t\ttorch.tensor(item, dtype=torch.long),\n",
        "\t\t\ttorch.tensor(rating, dtype=torch.float)\n",
        "\t\t\t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "FxZCDy4tYkRZ"
      },
      "outputs": [],
      "source": [
        "class NCF_Data(object):\n",
        "\t\"\"\"\n",
        "\tConstruct Dataset for NCF\n",
        "\t\"\"\"\n",
        "\tdef __init__(self, args, ratings):\n",
        "\t\tself.ratings = ratings\n",
        "\t\tself.num_ng = args.num_ng\n",
        "\t\tself.num_ng_test = args.num_ng_test\n",
        "\t\tself.batch_size = args.batch_size\n",
        "\n",
        "\t\tself.preprocess_ratings = self._reindex(self.ratings)\n",
        "\n",
        "\t\tself.user_pool = set(self.ratings['user_id'].unique())\n",
        "\t\tself.item_pool = set(self.ratings['item_id'].unique())\n",
        "\n",
        "\t\tself.train_ratings, self.test_ratings = self._leave_one_out(self.preprocess_ratings)\n",
        "\t\tself.negatives = self._negative_sampling(self.preprocess_ratings)\n",
        "\n",
        "\t\n",
        "\tdef _reindex(self, ratings):\n",
        "\t\t\"\"\"\n",
        "\t\tProcess dataset to reindex userID and itemID, also set rating as binary feedback\n",
        "\t\t\"\"\"\n",
        "\t\tuser_list = list(ratings['user_id'].drop_duplicates())\n",
        "\t\tuser2id = {w: i for i, w in enumerate(user_list)}\n",
        "\n",
        "\t\titem_list = list(ratings['item_id'].drop_duplicates())\n",
        "\t\titem2id = {w: i for i, w in enumerate(item_list)}\n",
        "\n",
        "\t\tratings['user_id'] = ratings['user_id'].apply(lambda x: user2id[x])\n",
        "\t\tratings['item_id'] = ratings['item_id'].apply(lambda x: item2id[x])\n",
        "\t\tratings['rating'] = ratings['rating'].apply(lambda x: float(x > 0))\n",
        "\t\treturn ratings\n",
        "\n",
        "\tdef _leave_one_out(self, ratings):\n",
        "\t\t\"\"\"\n",
        "\t\tleave-one-out evaluation protocol in paper https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf\n",
        "\t\t\"\"\"\n",
        "\t\tratings['rank_latest'] = ratings.groupby(['user_id'])['timestamp'].rank(method='first', ascending=False)\n",
        "\t\ttest = ratings.loc[ratings['rank_latest'] == 1]\n",
        "\t\ttrain = ratings.loc[ratings['rank_latest'] > 1]\n",
        "\t\tassert train['user_id'].nunique()==test['user_id'].nunique(), 'Not Match Train User with Test User'\n",
        "\t\treturn train[['user_id', 'item_id', 'rating']], test[['user_id', 'item_id', 'rating']]\n",
        "\n",
        "\tdef _negative_sampling(self, ratings):\n",
        "\t\tinteract_status = (\n",
        "\t\t\tratings.groupby('user_id')['item_id']\n",
        "\t\t\t.apply(set)\n",
        "\t\t\t.reset_index()\n",
        "\t\t\t.rename(columns={'item_id': 'interacted_items'}))\n",
        "\t\tinteract_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self.item_pool - x)\n",
        "\t\tinteract_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, self.num_ng_test))\n",
        "\t\treturn interact_status[['user_id', 'negative_items', 'negative_samples']]\n",
        "\n",
        "\tdef get_train_instance(self):\n",
        "\t\tusers, items, ratings = [], [], []\n",
        "\t\ttrain_ratings = pd.merge(self.train_ratings, self.negatives[['user_id', 'negative_items']], on='user_id')\n",
        "\t\ttrain_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, self.num_ng))\n",
        "\t\tfor row in train_ratings.itertuples():\n",
        "\t\t\tusers.append(int(row.user_id))\n",
        "\t\t\titems.append(int(row.item_id))\n",
        "\t\t\tratings.append(float(row.rating))\n",
        "\t\t\tfor i in range(self.num_ng):\n",
        "\t\t\t\tusers.append(int(row.user_id))\n",
        "\t\t\t\titems.append(int(row.negatives[i]))\n",
        "\t\t\t\tratings.append(float(0))  # negative samples get 0 rating\n",
        "\t\tdataset = Rating_Datset(\n",
        "\t\t\tuser_list=users,\n",
        "\t\t\titem_list=items,\n",
        "\t\t\trating_list=ratings)\n",
        "\t\treturn DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "\tdef get_test_instance(self):\n",
        "\t\tusers, items, ratings = [], [], []\n",
        "\t\ttest_ratings = pd.merge(self.test_ratings, self.negatives[['user_id', 'negative_samples']], on='user_id')\n",
        "\t\tfor row in test_ratings.itertuples():\n",
        "\t\t\tusers.append(int(row.user_id))\n",
        "\t\t\titems.append(int(row.item_id))\n",
        "\t\t\tratings.append(float(row.rating))\n",
        "\t\t\tfor i in getattr(row, 'negative_samples'):\n",
        "\t\t\t\tusers.append(int(row.user_id))\n",
        "\t\t\t\titems.append(int(i))\n",
        "\t\t\t\tratings.append(float(0))\n",
        "\t\tdataset = Rating_Datset(\n",
        "\t\t\tuser_list=users,\n",
        "\t\t\titem_list=items,\n",
        "\t\t\trating_list=ratings)\n",
        "\t\treturn DataLoader(dataset, batch_size=self.num_ng_test+1, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx2UpNblDKH0"
      },
      "source": [
        "### Evaluation Metrics\n",
        "\n",
        "We randomly sample 100 items that are not interacted by the user, ranking the test item among the 100 items. We truncate the ranked list at 10.\n",
        "\n",
        "Since it is too time-consuming to rank all items for every user, for we will have to calculate 1000\\*1700 ~10⁶ values. With this strategy, we need 1000*100 ~ 10⁵ values, an order of magnitude less.\n",
        "\n",
        "For each user, we use the latest rating(according to timestamp) in the test set, and we use the rest for training. This evaluation methodology is also known as leave-one-out strategy.\n",
        "\n",
        "#### Metrics\n",
        "\n",
        "We use **Hit Ratio** (HR), and **Normalized Discounted Cumulative Gain** (NDCG) to evaluate the performance for our RS.\n",
        "\n",
        "\n",
        "In recommender settings, the **HR** is simply the fraction of users for which the correct answer is included in the recommendation list of length L.\n",
        "\n",
        "$$HR = \\frac{|U_{hit}^{L}|}{U_{all}}$$\n",
        "\n",
        "Where $U_{hit}^{L}$ is the number of users for which the correct answer is included in the top L recommendation list, while, $U_{all}$ is the total number of user in the test dataset. Clearly the larger L is the larger HR become.\n",
        "\n",
        "**NDCG** is a measure of ranking quality. In information retrieval, it is often used to measure effectiveness of web search engine algorithms or related applications. Using a graded relevance scale of documents in a search-engine result set, DCG measures the usefulness, or gain, of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom, with the gain of each result discounted at lower ranks\n",
        "\n",
        "**Gain** for an item is essentialy the same as the relevance score, which can be numerical ratings like search results in Google which can be rated in scale from 1 to 5, or binary in case of implicit data where we only know if a user has consumed certain item or not. Naturally **Cumulative Gain** is defined as the sum of gains up to a position k in the recommendation list.\n",
        "\n",
        "$$CG(k) = \\sum_{i=1}^{k}G_i$$\n",
        "\n",
        "One obvious drawback of CG is that it does not take into account of ordering. By swapping the relative order of any two items, the CG would be unaffected. This is problematic when ranking order is important. For example, on Google Search results, you would obviously not like placing the most relevant web page at the bottom.\n",
        "\n",
        "To penalize highly relevant items being placed at the bottom, we introduce the **Discounted Cumulative Gaing** (DCG).\n",
        "\n",
        "$$DCG(k) = \\sum_{i=1}^{k} \\frac{G_i}{log_2(i+1)}$$\n",
        "\n",
        "By diving the gain by its rank, we sort of push the algorithm to place highly relevant items to the top to achieve the best DCG score.\n",
        "\n",
        "There is still a drawback of DCG score. It is that DCG score adds up with the length of recommendation list. Therefore, we cannot consistently compare the DCG score for system recommending top 5 and top 10 items, because the latter will have higher score not because its recommendation quality but pure length.\n",
        "\n",
        "We tackle this issue by introducing **Ideal Discounted Cumulative Gain** (IDCG). IDCG is the DCG score for the most ideal ranking, which is ranking the items top down according their relevance up to position k.\n",
        "\n",
        "$$IDCG(k) = \\sum_{i=1}^{|I(k)|} \\frac{G_i}{log_2(i+1)}$$\n",
        "\n",
        "Where $|I(k)|$ represent the ideal list of items up to position k.\n",
        "\n",
        "And NDCG is simply to normalize the DCG score by IDCG such that its value is always between 0 and 1 regardless of the length.\n",
        "\n",
        "$$NDCD(k) = \\frac{DCG(k)}{IDCG(k)}$$\n",
        "\n",
        "Our model gives a confidence score between 0 and 1 for each item present in the test set for a given user. The items are sorted in decreasing order of their score, and top 10 items are given as recommendation. If the test item (which is only one for each user) is present in this list, HR is one for this user, else it is zero. The final HR is reported after averaging for all users. A similar calculation is done for NDCG.\n",
        "\n",
        "While training, we will be minimizing the cross-entropy loss, which is the standard loss function for a classification problem. The real strength of RS lies in giving a ranked list of top-k items, which a user is most likely to interact. Think about why you mostly click on google search results only on the first page, and never go to other pages. Metrics like NDCG and HR help in capturing this phenomenon by indicating the quality of our ranked lists. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "uTaIX7H52i-a"
      },
      "outputs": [],
      "source": [
        "def hit(ng_item, pred_items):\n",
        "\tif ng_item in pred_items:\n",
        "\t\treturn 1\n",
        "\treturn 0\n",
        "\n",
        "\n",
        "def ndcg(ng_item, pred_items):\n",
        "\tif ng_item in pred_items:\n",
        "\t\tindex = pred_items.index(ng_item)\n",
        "\t\treturn np.reciprocal(np.log2(index+2))\n",
        "\treturn 0\n",
        "\n",
        "\n",
        "def metrics(model, test_loader, top_k, device):\n",
        "\tHR, NDCG = [], []\n",
        "\n",
        "\tfor user, item, label in test_loader:\n",
        "\t\tuser = user.to(device)\n",
        "\t\titem = item.to(device)\n",
        "\n",
        "\t\tpredictions = model(user, item)\n",
        "\t\t_, indices = torch.topk(predictions, top_k)\n",
        "\t\trecommends = torch.take(\n",
        "\t\t\t\titem, indices).cpu().numpy().tolist()\n",
        "\n",
        "\t\tng_item = item[0].item() # leave one-out evaluation has only one item per user\n",
        "\t\tHR.append(hit(ng_item, recommends))\n",
        "\t\tNDCG.append(ndcg(ng_item, recommends))\n",
        "\n",
        "\treturn np.mean(HR), np.mean(NDCG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuigI6hAj6Fm"
      },
      "source": [
        "## Generalized Matrix Factorization (GMF)\n",
        "\n",
        "Generally Matrix Factorization (MF) algorithms associates each user and item with a real-valued vector of latent features. \n",
        "\n",
        "Let $\\mathbf{p_u}$ and $\\mathbf{q_i}$ denote the latent vector for user u and item i, respectively; MF estimates an interaction y_{ui} as the inner product of $\\mathbf{p_u}$ and $\\mathbf{q_i}$:\n",
        "\n",
        "$$y_{ui} = f(u, i| \\mathbf{p_u}, \\mathbf{q_i}) = \\mathbf{p_u}^T\\mathbf{q_i} = \\sum_{k=1}^Kp_{uk}q_{ik} $$ \n",
        "\n",
        "Where K denotes the dimension of the latent space.\n",
        "\n",
        "MF models the dimension of the interaction of user and item latent factors, assuming each dimension of the latent space is independent of each other and linearly combining them  with the same weights. \n",
        "\n",
        "This imposes some limitation of MF caused by the use of a simple and fixed inner product to estimate complex user-item interactions in the low-dimensional latent space.\n",
        "\n",
        "In order to overcome this limitation, we can build a Generalized Matrix Factorization (GMF) algorithm where we can weight the linear combination of the element-wise product and use this with an activation function to learn a representation of the input insted of using a fixed one.\n",
        "\n",
        "Let the user latent vector $\\mathbf{p_u}$  be denoted as $\\mathbf{P}^T\\mathbf{v}_u^T$ and the item latent vector $\\mathbf{q_i}$ as $\\mathbf{Q}^T\\mathbf{v}_i^T$. \n",
        "\n",
        "The GMF can be expressed as \n",
        "$$Y_{ui} = a_{out}(\\mathbf{h}^T(\\mathbf{p_u}\\odot \\mathbf{q_i})$$\n",
        "where $a_{out}$ is the activation function and $\\mathbf{h}$ are the edge weights of the output layer. \n",
        "\n",
        "Intuitively, if we use an identity function as $a_{out}$ and enforce $\\mathbf{h}$ to be a uniform vector of 1, we can exactly recover the MF model.\n",
        "\n",
        "In the following we will use $\\mathbf{h}$ as linear layer and a $a_{out}$ as sigmoid function to learn $\\mathbf{h}$ weigths from data with the log loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "DTMHganmd5q1"
      },
      "outputs": [],
      "source": [
        "class GMF(nn.Module):\n",
        "    def __init__(self, args, num_users, num_items):\n",
        "        super(GMF, self).__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.factor_num = args.factor_num\n",
        "\n",
        "        self.embedding_user = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num)\n",
        "        self.embedding_item = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num)\n",
        "\n",
        "        self.affine_output = nn.Linear(in_features=self.factor_num, out_features=1)\n",
        "        self.logistic = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding = self.embedding_user(user_indices)\n",
        "        item_embedding = self.embedding_item(item_indices)\n",
        "        element_product = torch.mul(user_embedding, item_embedding)\n",
        "        logits = self.affine_output(element_product)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating.squeeze()\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSUXUsUS5OoW"
      },
      "source": [
        "## Neural Collaborative Filtering\n",
        "GMF learn user and item embedding separately. To empower the model it is possible to combine the features of two pathways by concatenating them and passing this concatenation to a Multi-Layer Perceptron (MLP) to learn the interaction between user and item latent features. This model is known as Neural Collaborative Filtering (NCF)\n",
        "\n",
        "The input to the model is userID and itemID, which is fed into an embedding layer. Thus, each user and item is given an embedding. Then there are multiple dense layers afterward, followed by a single neuron with a sigmoid activation.\n",
        "\n",
        "The output of the sigmoid neuron can be interpreted as the probability the user is likely to interact with an item.\n",
        "\n",
        "The model, we are going to implement is the following: \n",
        "<center>  <img src=\"https://drive.google.com/uc?export=view&id=1rL_8kkHIhSlQjWr8hNal4Tyog87-2kNP\" width=\"550\" height=\"350\"> </center> \n",
        "\n",
        "The user in item vectors are passed to an embedding layer that build a dense or latent vectors for the sparse inputs, from the input layer. The obtained latent vectors are fed into the multi-layer neural architecture, to map the latent vectors to the predicted probability scores. The layers are responsible to find the complex user-item relations from the data. \n",
        "\n",
        "The output layer produces the predicted score $y_(ui)$, i.e, how much is the probability that the user u will interact with the item i.\n",
        "\n",
        "A pointwise loss function is used to minimize the difference between the target value Y(ui) and the corresponding predicted value.\n",
        "\n",
        "Formally the model we are going to implement is the following:\n",
        "\n",
        "$$\\mathbf{z_1} = \\phi_1 (\\mathbf{p_u}, \\mathbf{q_i}) = \\begin{bmatrix}\n",
        "\\mathbf{p_u} \\\\ \\mathbf{q_i}\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "$$\\phi_L(\\mathbf{z}_{L-1}) = a_L(\\mathbf{W}_L^T\\mathbf{z}_{L-1} + \\mathbf{b}_L)$$\n",
        "\n",
        "$$ y_{ui} = \\sigma(\\mathbf{h}^T\\phi_L(\\mathbf{z}_{L-1}))$$\n",
        "\n",
        "where $\\mathbf{W}_x, \\mathbf{b}_x, a_x$ denotes the weight matrix, bias vector and activation function for the x-th layer's perceptron\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "sokr3I_8d5tS"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, args, num_users, num_items):\n",
        "        super(MLP, self).__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.factor_num = args.factor_num\n",
        "        self.layers = args.layers\n",
        "\n",
        "        self.embedding_user = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num)\n",
        "        self.embedding_item = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num)\n",
        "\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "        for idx, (in_size, out_size) in enumerate(zip(self.layers[:-1], self.layers[1:])):\n",
        "            self.fc_layers.append(nn.Linear(in_size, out_size))\n",
        "\n",
        "        self.affine_output = nn.Linear(in_features=self.layers[-1], out_features=1)\n",
        "        self.logistic = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding = self.embedding_user(user_indices)\n",
        "        item_embedding = self.embedding_item(item_indices)\n",
        "        vector = torch.cat([user_embedding, item_embedding], dim=-1)  # the concat latent vector\n",
        "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
        "            vector = self.fc_layers[idx](vector)\n",
        "            vector = nn.ReLU()(vector)\n",
        "            # vector = nn.BatchNorm1d()(vector)\n",
        "            # vector = nn.Dropout(p=0.5)(vector)\n",
        "        logits = self.affine_output(vector)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating.squeeze()\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBeQ40sWReOa"
      },
      "source": [
        "## Task1: Fusion of GMF and NCF Models\n",
        "\n",
        "So far we have developed GMF that applies a linear kernel to model the latent feature interactions and MLP that uses a non-linear kernel to learn the interaction function from data.\n",
        "\n",
        "We now fuse GMF and MLP under the NCF framework, so that they can mutually reinforce each other to better model the complex user-item interactions?\n",
        "\n",
        "A straightforward solution is to let GMF and MLP share the same embedding layer and then combine the outputs of their interaction functions.\n",
        "\n",
        "To provide flexibility to the fused model we want to allow GMF and MLP to learn separate embeddings and combine two models by concatenating their last hidden layer.\n",
        "\n",
        "Formally\n",
        "\n",
        "𝜙𝐺𝑀𝐹=𝐩𝐮𝐺⊙𝐪𝐢𝐺 \n",
        "𝜙𝑀𝐿𝑃=𝑎𝐿(𝐖𝑇𝐿(𝑎𝐿−1(...𝑎2(𝐖𝑇2[𝐩𝐮𝐪𝐢]+𝐛2)...))+𝐛𝐿) \n",
        "\n",
        "𝑦𝑢𝑖=𝜎(𝐡𝑇[ 𝜙𝐺𝑀𝐹𝜙𝑀𝐿𝑃]) \n",
        "\n",
        "that results in the following model architecture\n",
        "\n",
        "<center>  <img src=\"https://drive.google.com/uc?export=view&id=1gNLUpiQdbDPMdvfZYVs3lcou3cd4Favb\" width=\"550\" height=\"400\"> </center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "N476ubgYJBC4"
      },
      "outputs": [],
      "source": [
        "#Task 1\n",
        "\n",
        "class neuMF(nn.Module):\n",
        "    def __init__(self, args, num_users, num_items):\n",
        "        super(neuMF, self).__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.factor_num_gmf = args.factor_num\n",
        "        self.factor_num_mlp =  int(args.layers[0]/2)\n",
        "        self.layers = args.layers\n",
        "\n",
        "        self.embedding_user_GMF = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_gmf)\n",
        "        self.embedding_item_GMF = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_gmf)\n",
        "        \n",
        "        self.embedding_user_MLP = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mlp)\n",
        "        self.embedding_item_MLP = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mlp)\n",
        "\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "        for idx, (in_size, out_size) in enumerate(zip(self.layers[:-1], self.layers[1:])):\n",
        "            self.fc_layers.append(nn.Linear(in_size, out_size))\n",
        "\n",
        "        self.affine_output = nn.Linear(in_features=args.layers[-1] + self.factor_num_gmf, out_features=1)\n",
        "        self.logistic = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding_GMF = self.embedding_user_GMF(user_indices)\n",
        "        item_embedding_GMF = self.embedding_item_GMF(item_indices)\n",
        "\n",
        "        user_embedding_MLP = self.embedding_user_MLP(user_indices)\n",
        "        item_embedding_MLP = self.embedding_item_MLP(item_indices)\n",
        "\n",
        "        #GMF Layer\n",
        "        element_product =torch.mul(user_embedding_GMF, item_embedding_GMF)\n",
        "        \n",
        "        #NCF/MLP Layer\n",
        "        vector = torch.cat([user_embedding_MLP, item_embedding_MLP], dim=-1)  # the concat latent vector\n",
        "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
        "            vector = self.fc_layers[idx](vector)\n",
        "            vector = nn.ReLU()(vector)\n",
        "            # vector = nn.BatchNorm1d()(vector)\n",
        "            # vector = nn.Dropout(p=0.5)(vector)\n",
        "        \n",
        "        #Combined NeuMF Layer\n",
        "        #x = torch.add(linear_gmf, linear_mlp, alpha=1, out=None)\n",
        "        x = torch.cat([vector, element_product], dim=-1)\n",
        "        logits = self.affine_output(x)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating.squeeze()\n",
        "    \n",
        "    def init_weight(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u2Bg8YtSf0h"
      },
      "source": [
        "##Tuning Parameters\n",
        "\n",
        "Defining all the hyperparameters(tuning) parameters, based on which the performance of model will depend. Some of the important parameters involve, number of neurons of MLP each layer, number of predictive factors, etc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzFNcyild5y8"
      },
      "outputs": [],
      "source": [
        "#collapse-hide\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--seed\", \n",
        "\ttype=int, \n",
        "\tdefault=42, \n",
        "\thelp=\"Seed\")\n",
        "parser.add_argument(\"--lr\", \n",
        "\ttype=float, \n",
        "\tdefault=0.001, \n",
        "\thelp=\"learning rate\")\n",
        "parser.add_argument(\"--dropout\", \n",
        "\ttype=float,\n",
        "\tdefault=0.2,  \n",
        "\thelp=\"dropout rate\")\n",
        "parser.add_argument(\"--batch_size\", \n",
        "\ttype=int, \n",
        "\tdefault=256, \n",
        "\thelp=\"batch size for training\")\n",
        "parser.add_argument(\"--epochs\", \n",
        "\ttype=int,\n",
        "\tdefault=30,  \n",
        "\thelp=\"training epoches\")\n",
        "parser.add_argument(\"--top_k\", \n",
        "\ttype=int, \n",
        "\tdefault=10, \n",
        "\thelp=\"compute metrics@top_k\")\n",
        "parser.add_argument(\"--factor_num\", \n",
        "\ttype=int,\n",
        "\tdefault=32, \n",
        "\thelp=\"predictive factors numbers in the model\")\n",
        "parser.add_argument(\"--layers\",\n",
        "    nargs='+', \n",
        "    default=[64,32,16,8],\n",
        "    help=\"MLP layers. Note that the first layer is the concatenation of user \\\n",
        "    and item embeddings. So layers[0]/2 is the embedding size.\")\n",
        "parser.add_argument(\"--num_ng\", \n",
        "\ttype=int,\n",
        "\tdefault=4, \n",
        "\thelp=\"Number of negative samples for training set\")\n",
        "parser.add_argument(\"--num_ng_test\", \n",
        "\ttype=int,\n",
        "\tdefault=100, \n",
        "\thelp=\"Number of negative samples for test set\")\n",
        "parser.add_argument(\"--out\", \n",
        "\tdefault=True,\n",
        "\thelp=\"save model or not\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viYIB3UXfHvV"
      },
      "source": [
        "##Task 2: Performance Checking\n",
        "\n",
        "Script to check performance of GMF for predictive numbers [8,16,32,64].\n",
        "Here while tuning the predictive factors we also have to tune the \"layers\" parameter as well. The reason for tuning \"layers\" parameter as well is that, this parameters holds the number of neurons present in each layer. So when, for instance, if we make the predictive factor = 8, it means the input layer will take 8 embeddings of \"user\" and 8 embeddings of \"item\", and then the neuron, numbers will keep becoming half as we progress through layers. So the first layer of MLP, has to be 8+8=16 neurons, then 8 neurons in next layers and then 4 and 2 in subsequesnt layers. The same will happen when we tune the predictive factor to 16,32 and 64 respectively. To incorporate this, a loop running 4 times for each predictive factor is written here.\n",
        "\n",
        "#Performance testing of GMF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coXQgQWYd51r",
        "outputId": "03b58959-757e-4454-fe71-e5f6a1141442"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-87c186f5da76>:52: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, self.num_ng_test))\n",
            "<ipython-input-7-87c186f5da76>:58: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, self.num_ng))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i:\n",
            "8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 time to train: 00: 00: 24\n",
            "HR: 0.090\tNDCG: 0.040\n",
            "Epoch 002 time to train: 00: 00: 25\n",
            "HR: 0.092\tNDCG: 0.041\n",
            "Epoch 003 time to train: 00: 00: 25\n",
            "HR: 0.106\tNDCG: 0.049\n",
            "Epoch 004 time to train: 00: 00: 24\n",
            "HR: 0.153\tNDCG: 0.073\n",
            "Epoch 005 time to train: 00: 00: 24\n",
            "HR: 0.207\tNDCG: 0.103\n",
            "Epoch 006 time to train: 00: 00: 25\n",
            "HR: 0.281\tNDCG: 0.142\n",
            "Epoch 007 time to train: 00: 00: 24\n",
            "HR: 0.329\tNDCG: 0.172\n",
            "Epoch 008 time to train: 00: 00: 23\n",
            "HR: 0.355\tNDCG: 0.189\n",
            "Epoch 009 time to train: 00: 00: 24\n",
            "HR: 0.369\tNDCG: 0.195\n",
            "Epoch 010 time to train: 00: 00: 25\n",
            "HR: 0.368\tNDCG: 0.200\n",
            "Epoch 011 time to train: 00: 00: 24\n",
            "HR: 0.380\tNDCG: 0.205\n",
            "Epoch 012 time to train: 00: 00: 24\n",
            "HR: 0.386\tNDCG: 0.211\n",
            "Epoch 013 time to train: 00: 00: 25\n",
            "HR: 0.398\tNDCG: 0.219\n",
            "Epoch 014 time to train: 00: 00: 25\n",
            "HR: 0.404\tNDCG: 0.227\n",
            "Epoch 015 time to train: 00: 00: 24\n",
            "HR: 0.417\tNDCG: 0.237\n",
            "Epoch 016 time to train: 00: 00: 23\n",
            "HR: 0.422\tNDCG: 0.246\n",
            "Epoch 017 time to train: 00: 00: 24\n",
            "HR: 0.436\tNDCG: 0.255\n",
            "Epoch 018 time to train: 00: 00: 25\n",
            "HR: 0.453\tNDCG: 0.262\n",
            "Epoch 019 time to train: 00: 00: 22\n",
            "HR: 0.480\tNDCG: 0.274\n",
            "Epoch 020 time to train: 00: 00: 23\n",
            "HR: 0.493\tNDCG: 0.280\n",
            "Epoch 021 time to train: 00: 00: 21\n",
            "HR: 0.510\tNDCG: 0.289\n",
            "Epoch 022 time to train: 00: 00: 23\n",
            "HR: 0.527\tNDCG: 0.296\n",
            "Epoch 023 time to train: 00: 00: 22\n",
            "HR: 0.541\tNDCG: 0.305\n",
            "Epoch 024 time to train: 00: 00: 22\n",
            "HR: 0.548\tNDCG: 0.310\n",
            "Epoch 025 time to train: 00: 00: 22\n",
            "HR: 0.556\tNDCG: 0.316\n",
            "Epoch 026 time to train: 00: 00: 21\n",
            "HR: 0.560\tNDCG: 0.316\n",
            "Epoch 027 time to train: 00: 00: 23\n",
            "HR: 0.562\tNDCG: 0.320\n",
            "Epoch 028 time to train: 00: 00: 21\n",
            "HR: 0.571\tNDCG: 0.323\n",
            "Epoch 029 time to train: 00: 00: 23\n",
            "HR: 0.581\tNDCG: 0.326\n",
            "Epoch 030 time to train: 00: 00: 22\n",
            "HR: 0.587\tNDCG: 0.330\n",
            "i:\n",
            "16\n",
            "Epoch 001 time to train: 00: 00: 22\n",
            "HR: 0.106\tNDCG: 0.046\n",
            "Epoch 002 time to train: 00: 00: 23\n",
            "HR: 0.094\tNDCG: 0.041\n",
            "Epoch 003 time to train: 00: 00: 22\n",
            "HR: 0.085\tNDCG: 0.039\n",
            "Epoch 004 time to train: 00: 00: 23\n",
            "HR: 0.094\tNDCG: 0.042\n",
            "Epoch 005 time to train: 00: 00: 22\n",
            "HR: 0.137\tNDCG: 0.061\n",
            "Epoch 006 time to train: 00: 00: 22\n",
            "HR: 0.212\tNDCG: 0.097\n",
            "Epoch 007 time to train: 00: 00: 23\n",
            "HR: 0.275\tNDCG: 0.134\n",
            "Epoch 008 time to train: 00: 00: 21\n",
            "HR: 0.302\tNDCG: 0.151\n",
            "Epoch 009 time to train: 00: 00: 23\n",
            "HR: 0.331\tNDCG: 0.168\n",
            "Epoch 010 time to train: 00: 00: 23\n",
            "HR: 0.355\tNDCG: 0.183\n",
            "Epoch 011 time to train: 00: 00: 22\n",
            "HR: 0.371\tNDCG: 0.196\n",
            "Epoch 012 time to train: 00: 00: 23\n",
            "HR: 0.389\tNDCG: 0.209\n",
            "Epoch 013 time to train: 00: 00: 22\n",
            "HR: 0.403\tNDCG: 0.220\n",
            "Epoch 014 time to train: 00: 00: 25\n",
            "HR: 0.429\tNDCG: 0.233\n",
            "Epoch 015 time to train: 00: 00: 27\n",
            "HR: 0.448\tNDCG: 0.242\n",
            "Epoch 016 time to train: 00: 00: 22\n",
            "HR: 0.462\tNDCG: 0.249\n",
            "Epoch 017 time to train: 00: 00: 22\n",
            "HR: 0.472\tNDCG: 0.254\n",
            "Epoch 018 time to train: 00: 00: 23\n",
            "HR: 0.478\tNDCG: 0.260\n",
            "Epoch 019 time to train: 00: 00: 22\n",
            "HR: 0.491\tNDCG: 0.266\n",
            "Epoch 020 time to train: 00: 00: 23\n",
            "HR: 0.497\tNDCG: 0.271\n",
            "Epoch 021 time to train: 00: 00: 22\n",
            "HR: 0.503\tNDCG: 0.277\n",
            "Epoch 022 time to train: 00: 00: 23\n",
            "HR: 0.512\tNDCG: 0.283\n",
            "Epoch 023 time to train: 00: 00: 23\n",
            "HR: 0.525\tNDCG: 0.287\n",
            "Epoch 024 time to train: 00: 00: 21\n",
            "HR: 0.533\tNDCG: 0.290\n",
            "Epoch 025 time to train: 00: 00: 23\n",
            "HR: 0.543\tNDCG: 0.297\n",
            "Epoch 026 time to train: 00: 00: 21\n",
            "HR: 0.549\tNDCG: 0.299\n",
            "Epoch 027 time to train: 00: 00: 23\n",
            "HR: 0.555\tNDCG: 0.301\n",
            "Epoch 028 time to train: 00: 00: 23\n",
            "HR: 0.560\tNDCG: 0.306\n",
            "Epoch 029 time to train: 00: 00: 22\n",
            "HR: 0.563\tNDCG: 0.311\n",
            "Epoch 030 time to train: 00: 00: 24\n",
            "HR: 0.561\tNDCG: 0.311\n",
            "i:\n",
            "32\n",
            "Epoch 001 time to train: 00: 00: 23\n",
            "HR: 0.091\tNDCG: 0.042\n",
            "Epoch 002 time to train: 00: 00: 23\n",
            "HR: 0.086\tNDCG: 0.040\n",
            "Epoch 003 time to train: 00: 00: 24\n",
            "HR: 0.117\tNDCG: 0.056\n",
            "Epoch 004 time to train: 00: 00: 22\n",
            "HR: 0.182\tNDCG: 0.090\n",
            "Epoch 005 time to train: 00: 00: 23\n",
            "HR: 0.256\tNDCG: 0.126\n",
            "Epoch 006 time to train: 00: 00: 24\n",
            "HR: 0.298\tNDCG: 0.152\n",
            "Epoch 007 time to train: 00: 00: 22\n",
            "HR: 0.333\tNDCG: 0.173\n",
            "Epoch 008 time to train: 00: 00: 24\n",
            "HR: 0.352\tNDCG: 0.185\n",
            "Epoch 009 time to train: 00: 00: 24\n",
            "HR: 0.378\tNDCG: 0.202\n",
            "Epoch 010 time to train: 00: 00: 22\n",
            "HR: 0.394\tNDCG: 0.213\n",
            "Epoch 011 time to train: 00: 00: 24\n",
            "HR: 0.418\tNDCG: 0.227\n",
            "Epoch 012 time to train: 00: 00: 23\n",
            "HR: 0.448\tNDCG: 0.244\n",
            "Epoch 013 time to train: 00: 00: 23\n",
            "HR: 0.456\tNDCG: 0.252\n",
            "Epoch 014 time to train: 00: 00: 24\n",
            "HR: 0.477\tNDCG: 0.261\n",
            "Epoch 015 time to train: 00: 00: 22\n",
            "HR: 0.502\tNDCG: 0.274\n",
            "Epoch 016 time to train: 00: 00: 23\n",
            "HR: 0.515\tNDCG: 0.281\n",
            "Epoch 017 time to train: 00: 00: 24\n",
            "HR: 0.523\tNDCG: 0.286\n",
            "Epoch 018 time to train: 00: 00: 22\n",
            "HR: 0.524\tNDCG: 0.290\n",
            "Epoch 019 time to train: 00: 00: 24\n",
            "HR: 0.531\tNDCG: 0.294\n",
            "Epoch 020 time to train: 00: 00: 24\n",
            "HR: 0.534\tNDCG: 0.299\n",
            "Epoch 021 time to train: 00: 00: 22\n",
            "HR: 0.529\tNDCG: 0.296\n",
            "Epoch 022 time to train: 00: 00: 24\n",
            "HR: 0.536\tNDCG: 0.299\n",
            "Epoch 023 time to train: 00: 00: 24\n",
            "HR: 0.541\tNDCG: 0.300\n",
            "Epoch 024 time to train: 00: 00: 22\n",
            "HR: 0.544\tNDCG: 0.301\n",
            "Epoch 025 time to train: 00: 00: 24\n",
            "HR: 0.543\tNDCG: 0.300\n",
            "Epoch 026 time to train: 00: 00: 24\n",
            "HR: 0.557\tNDCG: 0.305\n",
            "Epoch 027 time to train: 00: 00: 22\n",
            "HR: 0.555\tNDCG: 0.305\n",
            "Epoch 028 time to train: 00: 00: 24\n",
            "HR: 0.556\tNDCG: 0.305\n",
            "Epoch 029 time to train: 00: 00: 23\n",
            "HR: 0.563\tNDCG: 0.308\n",
            "Epoch 030 time to train: 00: 00: 22\n",
            "HR: 0.564\tNDCG: 0.310\n",
            "i:\n",
            "64\n",
            "Epoch 001 time to train: 00: 00: 26\n",
            "HR: 0.091\tNDCG: 0.044\n",
            "Epoch 002 time to train: 00: 00: 25\n",
            "HR: 0.111\tNDCG: 0.050\n",
            "Epoch 003 time to train: 00: 00: 24\n",
            "HR: 0.138\tNDCG: 0.067\n",
            "Epoch 004 time to train: 00: 00: 24\n",
            "HR: 0.213\tNDCG: 0.106\n",
            "Epoch 005 time to train: 00: 00: 25\n",
            "HR: 0.274\tNDCG: 0.141\n",
            "Epoch 006 time to train: 00: 00: 25\n",
            "HR: 0.317\tNDCG: 0.165\n",
            "Epoch 007 time to train: 00: 00: 23\n",
            "HR: 0.340\tNDCG: 0.181\n",
            "Epoch 008 time to train: 00: 00: 25\n",
            "HR: 0.363\tNDCG: 0.192\n",
            "Epoch 009 time to train: 00: 00: 25\n",
            "HR: 0.389\tNDCG: 0.205\n",
            "Epoch 010 time to train: 00: 00: 25\n",
            "HR: 0.407\tNDCG: 0.215\n",
            "Epoch 011 time to train: 00: 00: 23\n",
            "HR: 0.427\tNDCG: 0.225\n",
            "Epoch 012 time to train: 00: 00: 25\n",
            "HR: 0.433\tNDCG: 0.231\n",
            "Epoch 013 time to train: 00: 00: 25\n",
            "HR: 0.440\tNDCG: 0.234\n",
            "Epoch 014 time to train: 00: 00: 25\n",
            "HR: 0.444\tNDCG: 0.237\n",
            "Epoch 015 time to train: 00: 00: 23\n",
            "HR: 0.449\tNDCG: 0.239\n",
            "Epoch 016 time to train: 00: 00: 26\n",
            "HR: 0.471\tNDCG: 0.247\n",
            "Epoch 017 time to train: 00: 00: 26\n",
            "HR: 0.469\tNDCG: 0.246\n",
            "Epoch 018 time to train: 00: 00: 25\n",
            "HR: 0.483\tNDCG: 0.252\n",
            "Epoch 019 time to train: 00: 00: 24\n",
            "HR: 0.492\tNDCG: 0.258\n",
            "Epoch 020 time to train: 00: 00: 25\n",
            "HR: 0.487\tNDCG: 0.256\n",
            "Epoch 021 time to train: 00: 00: 25\n",
            "HR: 0.487\tNDCG: 0.257\n",
            "Epoch 022 time to train: 00: 00: 25\n",
            "HR: 0.489\tNDCG: 0.260\n",
            "Epoch 023 time to train: 00: 00: 25\n",
            "HR: 0.483\tNDCG: 0.259\n",
            "Epoch 024 time to train: 00: 00: 24\n",
            "HR: 0.489\tNDCG: 0.264\n",
            "Epoch 025 time to train: 00: 00: 26\n",
            "HR: 0.489\tNDCG: 0.263\n",
            "Epoch 026 time to train: 00: 00: 25\n",
            "HR: 0.495\tNDCG: 0.267\n",
            "Epoch 027 time to train: 00: 00: 24\n",
            "HR: 0.489\tNDCG: 0.263\n",
            "Epoch 028 time to train: 00: 00: 24\n",
            "HR: 0.493\tNDCG: 0.267\n",
            "Epoch 029 time to train: 00: 00: 26\n",
            "HR: 0.497\tNDCG: 0.270\n",
            "Epoch 030 time to train: 00: 00: 25\n",
            "HR: 0.502\tNDCG: 0.272\n"
          ]
        }
      ],
      "source": [
        "## set device and parameters\n",
        "args = parser.parse_args(\"\")\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "hr_gmf,ndcg_gmf=[],[]\n",
        "hr_gmf_final,ndcg_gmf_final=[],[]\n",
        "\n",
        "# load data\n",
        "ml_100k = pd.read_csv(\n",
        "\tDATA_PATH, \n",
        "\tsep=\"\\t\", \n",
        "\tnames = ['user_id', 'item_id', 'rating', 'timestamp'], \n",
        "\tengine='python')\n",
        "ml_100k\n",
        "\n",
        "# set the num_users, items\n",
        "num_users = ml_100k['user_id'].nunique()+1\n",
        "num_items = ml_100k['item_id'].nunique()+1\n",
        "\n",
        "# construct the train and test datasets\n",
        "data = NCF_Data(args, ml_100k)\n",
        "train_loader = data.get_train_instance()\n",
        "test_loader = data.get_test_instance()\n",
        "\n",
        "predictive_factors = [8, 16, 32, 64]\n",
        "for i in predictive_factors:\n",
        "\targs.factor_num = i\n",
        "\targs.layers=[int(i*2),int(i),int(i/2),int(i/4)]\n",
        "\tprint(\"i:\")\n",
        "\tprint(i)\n",
        "\t# set model and loss, optimizer\n",
        "\tmodel = GMF(args, num_users, num_items) #For GMF\n",
        "\t#model = MLP(args, num_users, num_items) #For NCF/MLP\n",
        "\t#model = neuMF(args, num_users, num_items) # Task 1 model\n",
        "\tmodel.to(device)\n",
        "\tloss_function = nn.BCELoss()\n",
        "\toptimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "\t# train, evaluation\n",
        "\tbest_hr = 0\n",
        "\tfor epoch in range(1, args.epochs+1):\n",
        "\t\tmodel.train() # Enable dropout (if have).\n",
        "\t\tstart_time = time.time()\n",
        "\n",
        "\t\tfor user, item, label in train_loader:\n",
        "\t\t\tuser = user.to(device)\n",
        "\t\t\titem = item.to(device)\n",
        "\t\t\tlabel = label.to(device)\n",
        "\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\t\t\tprediction = model(user, item)\n",
        "\t\t\tloss = loss_function(prediction, label)\n",
        "\t\t\tloss.backward()\n",
        "\t\t\toptimizer.step()\n",
        "\t\t\t#writer.add_scalar('loss/Train_loss', loss.item(), epoch)\n",
        "\n",
        "\t\tmodel.eval()\n",
        "\t\tHR, NDCG = metrics(model, test_loader, args.top_k, device)\n",
        "\t\t#writer.add_scalar('Perfomance/HR@10', HR, epoch)\n",
        "\t\t#writer.add_scalar('Perfomance/NDCG@10', NDCG, epoch)\n",
        "\t\tnetHr = 0\n",
        "\t\telapsed_time = time.time() - start_time\n",
        "\t\tndcg_gmf.append(np.mean(NDCG))\n",
        "\t\thr_gmf.append(np.mean(HR))\n",
        "\t\tprint(\"Epoch {:03d}\".format(epoch) + \" time to train: \" + \n",
        "\t\t\t\ttime.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n",
        "\t\tprint(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))\n",
        "\t\n",
        "\n",
        "\t\tif HR > best_hr:\n",
        "\t\t\tbest_hr, best_ndcg, best_epoch = HR, NDCG, epoch\n",
        "\t\t\tif args.out:\n",
        "\t\t\t\tif not os.path.exists(MODEL_PATH):\n",
        "\t\t\t\t\tos.mkdir(MODEL_PATH)\n",
        "\t\t\t\ttorch.save(model, \n",
        "\t\t\t\t\t'{}{}.pt'.format(MODEL_PATH, model.__class__.__name__))\n",
        "\n",
        "\t\t\n",
        "\thr_gmf_final.append(hr_gmf[len(hr_gmf)-1:][0])\n",
        "\tndcg_gmf_final.append(ndcg_gmf[len(ndcg_gmf)-1:][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4d-v7l2UrVo"
      },
      "source": [
        "We are storing the last epoch's HR@10 result(the best output after error backpropagation in each epoch) in the hr_gmf_final array and NDCG@10 in ndcg_gmf_final array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lapoJ0AFGrbD",
        "outputId": "3d39592c-ee57-40df-a70e-6231f3b41dc1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.5874867444326617,\n",
              " 0.5609756097560976,\n",
              " 0.5641569459172853,\n",
              " 0.5015906680805938]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hr_gmf_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFUHoN4QgmZQ"
      },
      "source": [
        "#Code to evaluate the performance of MLP:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raJQUTKWe2qY",
        "outputId": "77f90c2a-b02d-43bf-8eaa-7a31b0f971f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-87c186f5da76>:52: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, self.num_ng_test))\n",
            "<ipython-input-7-87c186f5da76>:58: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, self.num_ng))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i:\n",
            "8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 time to train: 00: 00: 26\n",
            "HR: 0.003\tNDCG: 0.003\n",
            "Epoch 002 time to train: 00: 00: 27\n",
            "HR: 0.320\tNDCG: 0.168\n",
            "Epoch 003 time to train: 00: 00: 27\n",
            "HR: 0.340\tNDCG: 0.188\n",
            "Epoch 004 time to train: 00: 00: 26\n",
            "HR: 0.340\tNDCG: 0.188\n",
            "Epoch 005 time to train: 00: 00: 24\n",
            "HR: 0.347\tNDCG: 0.190\n",
            "Epoch 006 time to train: 00: 00: 26\n",
            "HR: 0.348\tNDCG: 0.191\n",
            "Epoch 007 time to train: 00: 00: 26\n",
            "HR: 0.356\tNDCG: 0.192\n",
            "Epoch 008 time to train: 00: 00: 26\n",
            "HR: 0.375\tNDCG: 0.201\n",
            "Epoch 009 time to train: 00: 00: 25\n",
            "HR: 0.371\tNDCG: 0.199\n",
            "Epoch 010 time to train: 00: 00: 25\n",
            "HR: 0.382\tNDCG: 0.205\n",
            "Epoch 011 time to train: 00: 00: 26\n",
            "HR: 0.382\tNDCG: 0.210\n",
            "Epoch 012 time to train: 00: 00: 26\n",
            "HR: 0.385\tNDCG: 0.209\n",
            "Epoch 013 time to train: 00: 00: 26\n",
            "HR: 0.382\tNDCG: 0.208\n",
            "Epoch 014 time to train: 00: 00: 25\n",
            "HR: 0.394\tNDCG: 0.216\n",
            "Epoch 015 time to train: 00: 00: 25\n",
            "HR: 0.400\tNDCG: 0.219\n",
            "Epoch 016 time to train: 00: 00: 26\n",
            "HR: 0.416\tNDCG: 0.232\n",
            "Epoch 017 time to train: 00: 00: 25\n",
            "HR: 0.424\tNDCG: 0.236\n",
            "Epoch 018 time to train: 00: 00: 25\n",
            "HR: 0.437\tNDCG: 0.239\n",
            "Epoch 019 time to train: 00: 00: 24\n",
            "HR: 0.440\tNDCG: 0.247\n",
            "Epoch 020 time to train: 00: 00: 25\n",
            "HR: 0.463\tNDCG: 0.259\n",
            "Epoch 021 time to train: 00: 00: 26\n",
            "HR: 0.473\tNDCG: 0.265\n",
            "Epoch 022 time to train: 00: 00: 26\n",
            "HR: 0.471\tNDCG: 0.267\n",
            "Epoch 023 time to train: 00: 00: 25\n",
            "HR: 0.489\tNDCG: 0.272\n",
            "Epoch 024 time to train: 00: 00: 24\n",
            "HR: 0.490\tNDCG: 0.272\n",
            "Epoch 025 time to train: 00: 00: 26\n",
            "HR: 0.496\tNDCG: 0.277\n",
            "Epoch 026 time to train: 00: 00: 26\n",
            "HR: 0.499\tNDCG: 0.278\n",
            "Epoch 027 time to train: 00: 00: 26\n",
            "HR: 0.498\tNDCG: 0.279\n",
            "Epoch 028 time to train: 00: 00: 25\n",
            "HR: 0.499\tNDCG: 0.280\n",
            "Epoch 029 time to train: 00: 00: 24\n",
            "HR: 0.503\tNDCG: 0.278\n",
            "Epoch 030 time to train: 00: 00: 26\n",
            "HR: 0.503\tNDCG: 0.280\n",
            "i:\n",
            "16\n",
            "Epoch 001 time to train: 00: 00: 26\n",
            "HR: 0.390\tNDCG: 0.206\n",
            "Epoch 002 time to train: 00: 00: 26\n",
            "HR: 0.397\tNDCG: 0.220\n",
            "Epoch 003 time to train: 00: 00: 26\n",
            "HR: 0.400\tNDCG: 0.220\n",
            "Epoch 004 time to train: 00: 00: 25\n",
            "HR: 0.410\tNDCG: 0.225\n",
            "Epoch 005 time to train: 00: 00: 26\n",
            "HR: 0.406\tNDCG: 0.224\n",
            "Epoch 006 time to train: 00: 00: 26\n",
            "HR: 0.408\tNDCG: 0.223\n",
            "Epoch 007 time to train: 00: 00: 26\n",
            "HR: 0.404\tNDCG: 0.227\n",
            "Epoch 008 time to train: 00: 00: 26\n",
            "HR: 0.400\tNDCG: 0.224\n",
            "Epoch 009 time to train: 00: 00: 25\n",
            "HR: 0.421\tNDCG: 0.232\n",
            "Epoch 010 time to train: 00: 00: 25\n",
            "HR: 0.426\tNDCG: 0.231\n",
            "Epoch 011 time to train: 00: 00: 26\n",
            "HR: 0.439\tNDCG: 0.236\n",
            "Epoch 012 time to train: 00: 00: 26\n",
            "HR: 0.438\tNDCG: 0.240\n",
            "Epoch 013 time to train: 00: 00: 26\n",
            "HR: 0.455\tNDCG: 0.247\n",
            "Epoch 014 time to train: 00: 00: 26\n",
            "HR: 0.463\tNDCG: 0.249\n",
            "Epoch 015 time to train: 00: 00: 24\n",
            "HR: 0.472\tNDCG: 0.257\n",
            "Epoch 016 time to train: 00: 00: 26\n",
            "HR: 0.475\tNDCG: 0.260\n",
            "Epoch 017 time to train: 00: 00: 27\n",
            "HR: 0.476\tNDCG: 0.257\n",
            "Epoch 018 time to train: 00: 00: 27\n",
            "HR: 0.473\tNDCG: 0.259\n",
            "Epoch 019 time to train: 00: 00: 26\n",
            "HR: 0.477\tNDCG: 0.263\n",
            "Epoch 020 time to train: 00: 00: 26\n",
            "HR: 0.479\tNDCG: 0.264\n",
            "Epoch 021 time to train: 00: 00: 26\n",
            "HR: 0.489\tNDCG: 0.267\n",
            "Epoch 022 time to train: 00: 00: 26\n",
            "HR: 0.492\tNDCG: 0.270\n",
            "Epoch 023 time to train: 00: 00: 26\n",
            "HR: 0.507\tNDCG: 0.279\n",
            "Epoch 024 time to train: 00: 00: 26\n",
            "HR: 0.502\tNDCG: 0.273\n",
            "Epoch 025 time to train: 00: 00: 26\n",
            "HR: 0.504\tNDCG: 0.277\n",
            "Epoch 026 time to train: 00: 00: 26\n",
            "HR: 0.516\tNDCG: 0.280\n",
            "Epoch 027 time to train: 00: 00: 25\n",
            "HR: 0.519\tNDCG: 0.282\n",
            "Epoch 028 time to train: 00: 00: 25\n",
            "HR: 0.522\tNDCG: 0.285\n",
            "Epoch 029 time to train: 00: 00: 26\n",
            "HR: 0.529\tNDCG: 0.284\n",
            "Epoch 030 time to train: 00: 00: 26\n",
            "HR: 0.516\tNDCG: 0.282\n",
            "i:\n",
            "32\n",
            "Epoch 001 time to train: 00: 00: 27\n",
            "HR: 0.405\tNDCG: 0.220\n",
            "Epoch 002 time to train: 00: 00: 27\n",
            "HR: 0.402\tNDCG: 0.220\n",
            "Epoch 003 time to train: 00: 00: 27\n",
            "HR: 0.399\tNDCG: 0.219\n",
            "Epoch 004 time to train: 00: 00: 26\n",
            "HR: 0.389\tNDCG: 0.214\n",
            "Epoch 005 time to train: 00: 00: 26\n",
            "HR: 0.405\tNDCG: 0.220\n",
            "Epoch 006 time to train: 00: 00: 27\n",
            "HR: 0.410\tNDCG: 0.227\n",
            "Epoch 007 time to train: 00: 00: 27\n",
            "HR: 0.434\tNDCG: 0.239\n",
            "Epoch 008 time to train: 00: 00: 27\n",
            "HR: 0.470\tNDCG: 0.256\n",
            "Epoch 009 time to train: 00: 00: 27\n",
            "HR: 0.475\tNDCG: 0.259\n",
            "Epoch 010 time to train: 00: 00: 27\n",
            "HR: 0.497\tNDCG: 0.269\n",
            "Epoch 011 time to train: 00: 00: 27\n",
            "HR: 0.503\tNDCG: 0.274\n",
            "Epoch 012 time to train: 00: 00: 27\n",
            "HR: 0.515\tNDCG: 0.282\n",
            "Epoch 013 time to train: 00: 00: 26\n",
            "HR: 0.522\tNDCG: 0.285\n",
            "Epoch 014 time to train: 00: 00: 26\n",
            "HR: 0.525\tNDCG: 0.289\n",
            "Epoch 015 time to train: 00: 00: 27\n",
            "HR: 0.526\tNDCG: 0.287\n",
            "Epoch 016 time to train: 00: 00: 27\n",
            "HR: 0.529\tNDCG: 0.288\n",
            "Epoch 017 time to train: 00: 00: 27\n",
            "HR: 0.546\tNDCG: 0.296\n",
            "Epoch 018 time to train: 00: 00: 27\n",
            "HR: 0.540\tNDCG: 0.300\n",
            "Epoch 019 time to train: 00: 00: 27\n",
            "HR: 0.549\tNDCG: 0.298\n",
            "Epoch 020 time to train: 00: 00: 27\n",
            "HR: 0.543\tNDCG: 0.300\n",
            "Epoch 021 time to train: 00: 00: 27\n",
            "HR: 0.547\tNDCG: 0.299\n",
            "Epoch 022 time to train: 00: 00: 27\n",
            "HR: 0.552\tNDCG: 0.305\n",
            "Epoch 023 time to train: 00: 00: 26\n",
            "HR: 0.547\tNDCG: 0.302\n",
            "Epoch 024 time to train: 00: 00: 26\n",
            "HR: 0.557\tNDCG: 0.303\n",
            "Epoch 025 time to train: 00: 00: 27\n",
            "HR: 0.555\tNDCG: 0.300\n",
            "Epoch 026 time to train: 00: 00: 27\n",
            "HR: 0.567\tNDCG: 0.307\n",
            "Epoch 027 time to train: 00: 00: 27\n",
            "HR: 0.541\tNDCG: 0.294\n",
            "Epoch 028 time to train: 00: 00: 28\n",
            "HR: 0.551\tNDCG: 0.298\n",
            "Epoch 029 time to train: 00: 00: 27\n",
            "HR: 0.546\tNDCG: 0.297\n",
            "Epoch 030 time to train: 00: 00: 27\n",
            "HR: 0.560\tNDCG: 0.301\n",
            "i:\n",
            "64\n",
            "Epoch 001 time to train: 00: 00: 28\n",
            "HR: 0.402\tNDCG: 0.223\n",
            "Epoch 002 time to train: 00: 00: 29\n",
            "HR: 0.403\tNDCG: 0.225\n",
            "Epoch 003 time to train: 00: 00: 29\n",
            "HR: 0.405\tNDCG: 0.222\n",
            "Epoch 004 time to train: 00: 00: 29\n",
            "HR: 0.446\tNDCG: 0.249\n",
            "Epoch 005 time to train: 00: 00: 28\n",
            "HR: 0.501\tNDCG: 0.283\n",
            "Epoch 006 time to train: 00: 00: 29\n",
            "HR: 0.524\tNDCG: 0.293\n",
            "Epoch 007 time to train: 00: 00: 29\n",
            "HR: 0.534\tNDCG: 0.294\n",
            "Epoch 008 time to train: 00: 00: 29\n",
            "HR: 0.554\tNDCG: 0.307\n",
            "Epoch 009 time to train: 00: 00: 29\n",
            "HR: 0.549\tNDCG: 0.307\n",
            "Epoch 010 time to train: 00: 00: 29\n",
            "HR: 0.559\tNDCG: 0.304\n",
            "Epoch 011 time to train: 00: 00: 29\n",
            "HR: 0.556\tNDCG: 0.306\n",
            "Epoch 012 time to train: 00: 00: 29\n",
            "HR: 0.544\tNDCG: 0.302\n",
            "Epoch 013 time to train: 00: 00: 29\n",
            "HR: 0.565\tNDCG: 0.308\n",
            "Epoch 014 time to train: 00: 00: 29\n",
            "HR: 0.551\tNDCG: 0.304\n",
            "Epoch 015 time to train: 00: 00: 29\n",
            "HR: 0.558\tNDCG: 0.305\n",
            "Epoch 016 time to train: 00: 00: 29\n",
            "HR: 0.555\tNDCG: 0.307\n",
            "Epoch 017 time to train: 00: 00: 28\n",
            "HR: 0.552\tNDCG: 0.302\n",
            "Epoch 018 time to train: 00: 00: 28\n",
            "HR: 0.559\tNDCG: 0.308\n",
            "Epoch 019 time to train: 00: 00: 29\n",
            "HR: 0.559\tNDCG: 0.303\n",
            "Epoch 020 time to train: 00: 00: 29\n",
            "HR: 0.556\tNDCG: 0.302\n",
            "Epoch 021 time to train: 00: 00: 29\n",
            "HR: 0.555\tNDCG: 0.307\n",
            "Epoch 022 time to train: 00: 00: 29\n",
            "HR: 0.559\tNDCG: 0.306\n",
            "Epoch 023 time to train: 00: 00: 29\n",
            "HR: 0.566\tNDCG: 0.306\n",
            "Epoch 024 time to train: 00: 00: 29\n",
            "HR: 0.558\tNDCG: 0.302\n",
            "Epoch 025 time to train: 00: 00: 28\n",
            "HR: 0.559\tNDCG: 0.306\n",
            "Epoch 026 time to train: 00: 00: 29\n",
            "HR: 0.562\tNDCG: 0.310\n",
            "Epoch 027 time to train: 00: 00: 29\n",
            "HR: 0.564\tNDCG: 0.309\n",
            "Epoch 028 time to train: 00: 00: 29\n",
            "HR: 0.549\tNDCG: 0.305\n",
            "Epoch 029 time to train: 00: 00: 29\n",
            "HR: 0.556\tNDCG: 0.302\n",
            "Epoch 030 time to train: 00: 00: 29\n",
            "HR: 0.561\tNDCG: 0.302\n"
          ]
        }
      ],
      "source": [
        "## set device and parameters\n",
        "args = parser.parse_args(\"\")\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "hr_mlp,ndcg_mlp=[],[]\n",
        "hr_mlp_final,ndcg_mlp_final=[],[]\n",
        "\n",
        "# load data\n",
        "ml_100k = pd.read_csv(\n",
        "\tDATA_PATH, \n",
        "\tsep=\"\\t\", \n",
        "\tnames = ['user_id', 'item_id', 'rating', 'timestamp'], \n",
        "\tengine='python')\n",
        "ml_100k\n",
        "\n",
        "# set the num_users, items\n",
        "num_users = ml_100k['user_id'].nunique()+1\n",
        "num_items = ml_100k['item_id'].nunique()+1\n",
        "\n",
        "# construct the train and test datasets\n",
        "data = NCF_Data(args, ml_100k)\n",
        "train_loader = data.get_train_instance()\n",
        "test_loader = data.get_test_instance()\n",
        "\n",
        "predictive_factors = [8, 16, 32, 64]\n",
        "for i in predictive_factors:\n",
        "\targs.factor_num = i\n",
        "\targs.layers=[int(i*2),int(i),int(i/2),int(i/4)]\n",
        "\tprint(\"i:\")\n",
        "\tprint(i)\n",
        "\t# set model and loss, optimizer\n",
        "\t#model = GMF(args, num_users, num_items) #For GMF\n",
        "\tmodel = MLP(args, num_users, num_items) #For NCF/MLP\n",
        "\t#model = neuMF(args, num_users, num_items) # Task 1 model\n",
        "\tmodel.to(device)\n",
        "\tloss_function = nn.BCELoss()\n",
        "\toptimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "\t# train, evaluation\n",
        "\tbest_hr = 0\n",
        "\tfor epoch in range(1, args.epochs+1):\n",
        "\t\tmodel.train() # Enable dropout (if have).\n",
        "\t\tstart_time = time.time()\n",
        "\n",
        "\t\tfor user, item, label in train_loader:\n",
        "\t\t\tuser = user.to(device)\n",
        "\t\t\titem = item.to(device)\n",
        "\t\t\tlabel = label.to(device)\n",
        "\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\t\t\tprediction = model(user, item)\n",
        "\t\t\tloss = loss_function(prediction, label)\n",
        "\t\t\tloss.backward()\n",
        "\t\t\toptimizer.step()\n",
        "\t\t\t#writer.add_scalar('loss/Train_loss', loss.item(), epoch)\n",
        "\n",
        "\t\tmodel.eval()\n",
        "\t\tHR, NDCG = metrics(model, test_loader, args.top_k, device)\n",
        "\t\t#writer.add_scalar('Perfomance/HR@10', HR, epoch)\n",
        "\t\t#writer.add_scalar('Perfomance/NDCG@10', NDCG, epoch)\n",
        "\t\tnetHr = 0\n",
        "\t\telapsed_time = time.time() - start_time\n",
        "\t\tndcg_mlp.append(np.mean(NDCG))\n",
        "\t\thr_mlp.append(np.mean(HR))\n",
        "\t\tprint(\"Epoch {:03d}\".format(epoch) + \" time to train: \" + \n",
        "\t\t\t\ttime.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n",
        "\t\tprint(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))\n",
        "\t\n",
        "\n",
        "\t\tif HR > best_hr:\n",
        "\t\t\tbest_hr, best_ndcg, best_epoch = HR, NDCG, epoch\n",
        "\t\t\tif args.out:\n",
        "\t\t\t\tif not os.path.exists(MODEL_PATH):\n",
        "\t\t\t\t\tos.mkdir(MODEL_PATH)\n",
        "\t\t\t\ttorch.save(model, \n",
        "\t\t\t\t\t'{}{}.pt'.format(MODEL_PATH, model.__class__.__name__))\n",
        "\t\t\n",
        "\thr_mlp_final.append(hr_mlp[len(hr_mlp)-1:][0])\n",
        "\tndcg_mlp_final.append(ndcg_mlp[len(ndcg_mlp)-1:][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SODoKPFDVTkV"
      },
      "source": [
        "We are storing the last epoch's HR@10 result(the best output after error backpropagation in each epoch) in the hr_mlp_final array and NDCG@10 in ndcg_mlp_final array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpREV_eU5v8g",
        "outputId": "b477fd0b-c7a7-410b-c2a4-f8d51b3454e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.5026511134676565, 0.5164369034994698, 0.559915164369035, 0.5609756097560976]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hr_mlp_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ve7XiachKhy"
      },
      "source": [
        "#Code to evaluate performance of neuMF Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxgobZpwg72t",
        "outputId": "ebc1add6-861f-4af5-e525-1ab06f8de5c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-29-87c186f5da76>:52: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, self.num_ng_test))\n",
            "<ipython-input-29-87c186f5da76>:58: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, self.num_ng))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i:\n",
            "8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 time to train: 00: 00: 30\n",
            "HR: 0.112\tNDCG: 0.051\n",
            "Epoch 002 time to train: 00: 00: 31\n",
            "HR: 0.329\tNDCG: 0.183\n",
            "Epoch 003 time to train: 00: 00: 29\n",
            "HR: 0.345\tNDCG: 0.197\n",
            "Epoch 004 time to train: 00: 00: 29\n",
            "HR: 0.349\tNDCG: 0.196\n",
            "Epoch 005 time to train: 00: 00: 30\n",
            "HR: 0.337\tNDCG: 0.193\n",
            "Epoch 006 time to train: 00: 00: 30\n",
            "HR: 0.351\tNDCG: 0.197\n",
            "Epoch 007 time to train: 00: 00: 29\n",
            "HR: 0.362\tNDCG: 0.202\n",
            "Epoch 008 time to train: 00: 00: 29\n",
            "HR: 0.371\tNDCG: 0.205\n",
            "Epoch 009 time to train: 00: 00: 29\n",
            "HR: 0.382\tNDCG: 0.210\n",
            "Epoch 010 time to train: 00: 00: 29\n",
            "HR: 0.388\tNDCG: 0.212\n",
            "Epoch 011 time to train: 00: 00: 29\n",
            "HR: 0.397\tNDCG: 0.220\n",
            "Epoch 012 time to train: 00: 00: 29\n",
            "HR: 0.399\tNDCG: 0.219\n",
            "Epoch 013 time to train: 00: 00: 29\n",
            "HR: 0.417\tNDCG: 0.230\n",
            "Epoch 014 time to train: 00: 00: 29\n",
            "HR: 0.422\tNDCG: 0.231\n",
            "Epoch 015 time to train: 00: 00: 29\n",
            "HR: 0.427\tNDCG: 0.233\n",
            "Epoch 016 time to train: 00: 00: 28\n",
            "HR: 0.434\tNDCG: 0.239\n",
            "Epoch 017 time to train: 00: 00: 29\n",
            "HR: 0.433\tNDCG: 0.243\n",
            "Epoch 018 time to train: 00: 00: 29\n",
            "HR: 0.438\tNDCG: 0.243\n",
            "Epoch 019 time to train: 00: 00: 29\n",
            "HR: 0.454\tNDCG: 0.252\n",
            "Epoch 020 time to train: 00: 00: 29\n",
            "HR: 0.469\tNDCG: 0.256\n",
            "Epoch 021 time to train: 00: 00: 32\n",
            "HR: 0.474\tNDCG: 0.262\n",
            "Epoch 022 time to train: 00: 00: 29\n",
            "HR: 0.477\tNDCG: 0.267\n",
            "Epoch 023 time to train: 00: 00: 29\n",
            "HR: 0.475\tNDCG: 0.267\n",
            "Epoch 024 time to train: 00: 00: 29\n",
            "HR: 0.484\tNDCG: 0.273\n",
            "Epoch 025 time to train: 00: 00: 29\n",
            "HR: 0.492\tNDCG: 0.279\n",
            "Epoch 026 time to train: 00: 00: 29\n",
            "HR: 0.507\tNDCG: 0.285\n",
            "Epoch 027 time to train: 00: 00: 29\n",
            "HR: 0.504\tNDCG: 0.289\n",
            "Epoch 028 time to train: 00: 00: 29\n",
            "HR: 0.509\tNDCG: 0.291\n",
            "Epoch 029 time to train: 00: 00: 30\n",
            "HR: 0.522\tNDCG: 0.297\n",
            "Epoch 030 time to train: 00: 00: 29\n",
            "HR: 0.516\tNDCG: 0.294\n",
            "i:\n",
            "16\n",
            "Epoch 001 time to train: 00: 00: 30\n",
            "HR: 0.384\tNDCG: 0.207\n",
            "Epoch 002 time to train: 00: 00: 29\n",
            "HR: 0.406\tNDCG: 0.219\n",
            "Epoch 003 time to train: 00: 00: 29\n",
            "HR: 0.401\tNDCG: 0.218\n",
            "Epoch 004 time to train: 00: 00: 29\n",
            "HR: 0.403\tNDCG: 0.216\n",
            "Epoch 005 time to train: 00: 00: 29\n",
            "HR: 0.398\tNDCG: 0.215\n",
            "Epoch 006 time to train: 00: 00: 29\n",
            "HR: 0.404\tNDCG: 0.218\n",
            "Epoch 007 time to train: 00: 00: 29\n",
            "HR: 0.400\tNDCG: 0.216\n",
            "Epoch 008 time to train: 00: 00: 29\n",
            "HR: 0.402\tNDCG: 0.222\n",
            "Epoch 009 time to train: 00: 00: 30\n",
            "HR: 0.405\tNDCG: 0.227\n",
            "Epoch 010 time to train: 00: 00: 29\n",
            "HR: 0.408\tNDCG: 0.229\n",
            "Epoch 011 time to train: 00: 00: 29\n",
            "HR: 0.434\tNDCG: 0.241\n",
            "Epoch 012 time to train: 00: 00: 29\n",
            "HR: 0.452\tNDCG: 0.248\n",
            "Epoch 013 time to train: 00: 00: 29\n",
            "HR: 0.463\tNDCG: 0.259\n",
            "Epoch 014 time to train: 00: 00: 29\n",
            "HR: 0.470\tNDCG: 0.267\n",
            "Epoch 015 time to train: 00: 00: 28\n",
            "HR: 0.462\tNDCG: 0.266\n",
            "Epoch 016 time to train: 00: 00: 29\n",
            "HR: 0.477\tNDCG: 0.276\n",
            "Epoch 017 time to train: 00: 00: 29\n",
            "HR: 0.495\tNDCG: 0.282\n",
            "Epoch 018 time to train: 00: 00: 29\n",
            "HR: 0.517\tNDCG: 0.287\n",
            "Epoch 019 time to train: 00: 00: 29\n",
            "HR: 0.516\tNDCG: 0.292\n",
            "Epoch 020 time to train: 00: 00: 29\n",
            "HR: 0.530\tNDCG: 0.297\n",
            "Epoch 021 time to train: 00: 00: 29\n",
            "HR: 0.548\tNDCG: 0.302\n",
            "Epoch 022 time to train: 00: 00: 29\n",
            "HR: 0.543\tNDCG: 0.303\n",
            "Epoch 023 time to train: 00: 00: 29\n",
            "HR: 0.536\tNDCG: 0.305\n",
            "Epoch 024 time to train: 00: 00: 29\n",
            "HR: 0.560\tNDCG: 0.310\n",
            "Epoch 025 time to train: 00: 00: 29\n",
            "HR: 0.555\tNDCG: 0.308\n",
            "Epoch 026 time to train: 00: 00: 29\n",
            "HR: 0.559\tNDCG: 0.307\n",
            "Epoch 027 time to train: 00: 00: 30\n",
            "HR: 0.571\tNDCG: 0.310\n",
            "Epoch 028 time to train: 00: 00: 30\n",
            "HR: 0.557\tNDCG: 0.307\n",
            "Epoch 029 time to train: 00: 00: 30\n",
            "HR: 0.563\tNDCG: 0.313\n",
            "Epoch 030 time to train: 00: 00: 30\n",
            "HR: 0.558\tNDCG: 0.316\n",
            "i:\n",
            "32\n",
            "Epoch 001 time to train: 00: 00: 30\n",
            "HR: 0.398\tNDCG: 0.224\n",
            "Epoch 002 time to train: 00: 00: 30\n",
            "HR: 0.400\tNDCG: 0.221\n",
            "Epoch 003 time to train: 00: 00: 29\n",
            "HR: 0.394\tNDCG: 0.221\n",
            "Epoch 004 time to train: 00: 00: 29\n",
            "HR: 0.399\tNDCG: 0.225\n",
            "Epoch 005 time to train: 00: 00: 30\n",
            "HR: 0.406\tNDCG: 0.230\n",
            "Epoch 006 time to train: 00: 00: 31\n",
            "HR: 0.428\tNDCG: 0.242\n",
            "Epoch 007 time to train: 00: 00: 30\n",
            "HR: 0.445\tNDCG: 0.256\n",
            "Epoch 008 time to train: 00: 00: 30\n",
            "HR: 0.479\tNDCG: 0.269\n",
            "Epoch 009 time to train: 00: 00: 31\n",
            "HR: 0.496\tNDCG: 0.277\n",
            "Epoch 010 time to train: 00: 00: 29\n",
            "HR: 0.495\tNDCG: 0.280\n",
            "Epoch 011 time to train: 00: 00: 29\n",
            "HR: 0.502\tNDCG: 0.280\n",
            "Epoch 012 time to train: 00: 00: 28\n",
            "HR: 0.505\tNDCG: 0.285\n",
            "Epoch 013 time to train: 00: 00: 29\n",
            "HR: 0.499\tNDCG: 0.278\n",
            "Epoch 014 time to train: 00: 00: 29\n",
            "HR: 0.503\tNDCG: 0.285\n",
            "Epoch 015 time to train: 00: 00: 29\n",
            "HR: 0.509\tNDCG: 0.285\n",
            "Epoch 016 time to train: 00: 00: 29\n",
            "HR: 0.523\tNDCG: 0.291\n",
            "Epoch 017 time to train: 00: 00: 29\n",
            "HR: 0.512\tNDCG: 0.291\n",
            "Epoch 018 time to train: 00: 00: 29\n",
            "HR: 0.532\tNDCG: 0.292\n",
            "Epoch 019 time to train: 00: 00: 28\n",
            "HR: 0.528\tNDCG: 0.293\n",
            "Epoch 020 time to train: 00: 00: 28\n",
            "HR: 0.522\tNDCG: 0.288\n",
            "Epoch 021 time to train: 00: 00: 29\n",
            "HR: 0.520\tNDCG: 0.293\n",
            "Epoch 022 time to train: 00: 00: 29\n",
            "HR: 0.526\tNDCG: 0.287\n",
            "Epoch 023 time to train: 00: 00: 29\n",
            "HR: 0.529\tNDCG: 0.291\n",
            "Epoch 024 time to train: 00: 00: 29\n",
            "HR: 0.522\tNDCG: 0.292\n",
            "Epoch 025 time to train: 00: 00: 29\n",
            "HR: 0.533\tNDCG: 0.296\n",
            "Epoch 026 time to train: 00: 00: 29\n",
            "HR: 0.534\tNDCG: 0.295\n",
            "Epoch 027 time to train: 00: 00: 29\n",
            "HR: 0.527\tNDCG: 0.293\n",
            "Epoch 028 time to train: 00: 00: 29\n",
            "HR: 0.526\tNDCG: 0.291\n",
            "Epoch 029 time to train: 00: 00: 29\n",
            "HR: 0.539\tNDCG: 0.293\n",
            "Epoch 030 time to train: 00: 00: 29\n",
            "HR: 0.527\tNDCG: 0.295\n",
            "i:\n",
            "64\n",
            "Epoch 001 time to train: 00: 00: 30\n",
            "HR: 0.387\tNDCG: 0.216\n",
            "Epoch 002 time to train: 00: 00: 30\n",
            "HR: 0.394\tNDCG: 0.215\n",
            "Epoch 003 time to train: 00: 00: 32\n",
            "HR: 0.402\tNDCG: 0.220\n",
            "Epoch 004 time to train: 00: 00: 31\n",
            "HR: 0.460\tNDCG: 0.258\n",
            "Epoch 005 time to train: 00: 00: 30\n",
            "HR: 0.490\tNDCG: 0.279\n",
            "Epoch 006 time to train: 00: 00: 30\n",
            "HR: 0.525\tNDCG: 0.294\n",
            "Epoch 007 time to train: 00: 00: 30\n",
            "HR: 0.551\tNDCG: 0.309\n",
            "Epoch 008 time to train: 00: 00: 30\n",
            "HR: 0.568\tNDCG: 0.316\n",
            "Epoch 009 time to train: 00: 00: 30\n",
            "HR: 0.566\tNDCG: 0.319\n",
            "Epoch 010 time to train: 00: 00: 32\n",
            "HR: 0.577\tNDCG: 0.324\n",
            "Epoch 011 time to train: 00: 00: 31\n",
            "HR: 0.582\tNDCG: 0.329\n",
            "Epoch 012 time to train: 00: 00: 31\n",
            "HR: 0.580\tNDCG: 0.324\n",
            "Epoch 013 time to train: 00: 00: 31\n",
            "HR: 0.576\tNDCG: 0.329\n",
            "Epoch 014 time to train: 00: 00: 30\n",
            "HR: 0.580\tNDCG: 0.329\n",
            "Epoch 015 time to train: 00: 00: 31\n",
            "HR: 0.578\tNDCG: 0.336\n",
            "Epoch 016 time to train: 00: 00: 31\n",
            "HR: 0.582\tNDCG: 0.330\n",
            "Epoch 017 time to train: 00: 00: 31\n",
            "HR: 0.597\tNDCG: 0.333\n",
            "Epoch 018 time to train: 00: 00: 31\n",
            "HR: 0.579\tNDCG: 0.328\n",
            "Epoch 019 time to train: 00: 00: 31\n",
            "HR: 0.575\tNDCG: 0.328\n",
            "Epoch 020 time to train: 00: 00: 31\n",
            "HR: 0.574\tNDCG: 0.330\n",
            "Epoch 021 time to train: 00: 00: 31\n",
            "HR: 0.568\tNDCG: 0.323\n",
            "Epoch 022 time to train: 00: 00: 32\n",
            "HR: 0.586\tNDCG: 0.331\n",
            "Epoch 023 time to train: 00: 00: 31\n",
            "HR: 0.590\tNDCG: 0.335\n",
            "Epoch 024 time to train: 00: 00: 31\n",
            "HR: 0.585\tNDCG: 0.327\n",
            "Epoch 025 time to train: 00: 00: 31\n",
            "HR: 0.584\tNDCG: 0.329\n",
            "Epoch 026 time to train: 00: 00: 31\n",
            "HR: 0.584\tNDCG: 0.323\n",
            "Epoch 027 time to train: 00: 00: 31\n",
            "HR: 0.591\tNDCG: 0.333\n",
            "Epoch 028 time to train: 00: 00: 32\n",
            "HR: 0.582\tNDCG: 0.326\n",
            "Epoch 029 time to train: 00: 00: 30\n",
            "HR: 0.578\tNDCG: 0.325\n",
            "Epoch 030 time to train: 00: 00: 31\n",
            "HR: 0.586\tNDCG: 0.326\n"
          ]
        }
      ],
      "source": [
        "## set device and parameters\n",
        "args = parser.parse_args(\"\")\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "hr_neuMF,ndcg_neuMF=[],[]\n",
        "hr_neuMF_final,ndcg_neuMF_final=[],[]\n",
        "\n",
        "# load data\n",
        "ml_100k = pd.read_csv(\n",
        "\tDATA_PATH, \n",
        "\tsep=\"\\t\", \n",
        "\tnames = ['user_id', 'item_id', 'rating', 'timestamp'], \n",
        "\tengine='python')\n",
        "ml_100k\n",
        "\n",
        "# set the num_users, items\n",
        "num_users = ml_100k['user_id'].nunique()+1\n",
        "num_items = ml_100k['item_id'].nunique()+1\n",
        "\n",
        "# construct the train and test datasets\n",
        "data = NCF_Data(args, ml_100k)\n",
        "train_loader = data.get_train_instance()\n",
        "test_loader = data.get_test_instance()\n",
        "\n",
        "predictive_factors = [8, 16, 32, 64]\n",
        "for i in predictive_factors:\n",
        "\targs.factor_num = i\n",
        "\targs.layers=[int(i*2),int(i),int(i/2),int(i/4)]\n",
        "\tprint(\"i:\")\n",
        "\tprint(i)\n",
        "\t# set model and loss, optimizer\n",
        "\t#model = GMF(args, num_users, num_items) #For GMF\n",
        "\t#model = MLP(args, num_users, num_items) #For NCF/MLP\n",
        "\tmodel = neuMF(args, num_users, num_items) # Task 1 model\n",
        "\tmodel.to(device)\n",
        "\tloss_function = nn.BCELoss()\n",
        "\toptimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "\t# train, evaluation\n",
        "\tbest_hr = 0\n",
        "\tfor epoch in range(1, args.epochs+1):\n",
        "\t\tmodel.train() # Enable dropout (if have).\n",
        "\t\tstart_time = time.time()\n",
        "\n",
        "\t\tfor user, item, label in train_loader:\n",
        "\t\t\tuser = user.to(device)\n",
        "\t\t\titem = item.to(device)\n",
        "\t\t\tlabel = label.to(device)\n",
        "\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\t\t\tprediction = model(user, item)\n",
        "\t\t\tloss = loss_function(prediction, label)\n",
        "\t\t\tloss.backward()\n",
        "\t\t\toptimizer.step()\n",
        "\t\t\t#writer.add_scalar('loss/Train_loss', loss.item(), epoch)\n",
        "\n",
        "\t\tmodel.eval()\n",
        "\t\tHR, NDCG = metrics(model, test_loader, args.top_k, device)\n",
        "\t\t#writer.add_scalar('Perfomance/HR@10', HR, epoch)\n",
        "\t\t#writer.add_scalar('Perfomance/NDCG@10', NDCG, epoch)\n",
        "\t\tnetHr = 0\n",
        "\t\telapsed_time = time.time() - start_time\n",
        "\t\tndcg_neuMF.append(np.mean(NDCG))\n",
        "\t\thr_neuMF.append(np.mean(HR))\n",
        "\t\tprint(\"Epoch {:03d}\".format(epoch) + \" time to train: \" + \n",
        "\t\t\t\ttime.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n",
        "\t\tprint(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))\n",
        "\t\n",
        "\n",
        "\t\tif HR > best_hr:\n",
        "\t\t\tbest_hr, best_ndcg, best_epoch = HR, NDCG, epoch\n",
        "\t\t\tif args.out:\n",
        "\t\t\t\tif not os.path.exists(MODEL_PATH):\n",
        "\t\t\t\t\tos.mkdir(MODEL_PATH)\n",
        "\t\t\t\ttorch.save(model, \n",
        "\t\t\t\t\t'{}{}.pt'.format(MODEL_PATH, model.__class__.__name__))\n",
        "\t\t\n",
        "\thr_neuMF_final.append(hr_neuMF[len(hr_neuMF)-1:][0])\n",
        "\tndcg_neuMF_final.append(ndcg_neuMF[len(ndcg_neuMF)-1:][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4kSKdcUVcvU"
      },
      "source": [
        "We are storing the last epoch's HR@10 result(the best output after error backpropagation in each epoch) in the hr_neuMF_final array and NDCG@10 in ndcg_neuMF_final array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNnZl5GUEUxs",
        "outputId": "66764d17-2461-4e5a-90d1-e81f91070054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.5164369034994698,\n",
              " 0.5577942735949099,\n",
              " 0.5270413573700954,\n",
              " 0.5864262990455992]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hr_neuMF_final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0AWPOfDVG1n",
        "outputId": "5fd242db-9a23-4563-ddea-58ab905bd7ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.29382915023384887,\n",
              " 0.3161151260590204,\n",
              " 0.2946336833471362,\n",
              " 0.32601119491046215]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ndcg_neuMF_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsILZanOVjV8"
      },
      "source": [
        "##TASK 3: \n",
        "tune the networks by plotting HR@10 and NDCG@10 with respect to the number of predictive factors [8, 16, 32, 64] for all the 3 algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "6nbtDGXUrCsB",
        "outputId": "e6bfeece-3209-4f29-b614-a0b455382141"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAABanUlEQVR4nO3dd3hU1dbA4d9KhyS00Iv03kKRDoKFLkURQhNQBBXEBgroh8K1gKhXvTQRJdISqoBUC9IR6Z3QkYBA6KQQUvb3x5zgECakzcyZJPt9nnmYOXXtmXDWqWuLUgpN0zRNS87N7AA0TdM016QThKZpmmaTThCapmmaTTpBaJqmaTbpBKFpmqbZpBOEpmmaZpNOEJqmaZpNOkFomglERIlIBeP9NBH5vwwuJ1JEytk3Ok2z0AlCe4CInBGRJ5MN6y8im5NNE2NsoC6KSLCI+NlYVmUR+V5ETovINRE5ICJjRcQ/2XStROQPEbkpImdsLKeMMT5aRI4mj8/eRKSliCQa7bstImEiMsAR61JKvayU+k8aYlovIgOTzeunlDplz3iStT3p9XMmlhcsIh/ZM0bNOXSC0DLjaaWUHxAI1AFGWY8UkWeA1cBuoCkQAHQEFLBdRB6xmjwK+AEYkcK6QoA9xjLeAxaJSCG7tcS2C0b78gDvAt+JSLXkE4mIh4PjMMMFI/kkvZ42KxARcTdr3TmdThBapimlLgJrsSQKwHLkAHwGtFBKTVZKXVAWZ5VSH2JJBDOtlvGXUmo28MDesIhUAuoCHyilYpRSi4EDwLM2pm1oHNG4Ww3rKiL7jfcNRGSniNwSkUsi8mUa2qeUUkuB60A142hqi4j8V0SuAh+KiLeIfC4ifxvLnSYiuaxiGCEi/4jIBRF5IVnM9+1hi0hnEdlrxHhSRNqKyMdAc2CSsUc/yZhWiUiFNLTbTURGGsu7KiILRKRAam1PFmcHEdljxHVORD5MNr6ZiGwVkRvG+P4iMgjoDbxjfSQiIlWNI6IbInJIRDol+z6misgqEYkCWolIexE5bBzNnReR4emJXcsYnSC0TBORkkA74ITV4JHA/ymlwkWku3GK6ayIvCci3ymlVgKJIlIjDauoDpxSSt22GrbPGH4fpdR2LEcjj1sN7gXMM95/DXytlMoDlAcWpKF9biLSFciHJTEBNMSSzIoAHwPjgUpYkmQFoAQwxpi/LTAceAqoCKR4ekxEGgCzsCTQfEAL4IxS6j1gEzDU2KMfms52vwZ0AR4DimNJdpNTa3syUcDzRlwdgFdEpIsRd2ksR4v/Awph+R72KqWmA3OBz5KORETEE/gZ+AUobMQ219ipsI79Y8Af2Ax8DwxWSvkDNYB16YxdywCdILSULDX27m6IyA1gSgrT3AbOAZeBD6zGtQSWGHupU4BuWDaglQBPY5q9QJU0xOIH3Ew27CaWjYctIUBPALFc62hvDAOIAyqISEGlVKRS6s+HrLe40fYrWNrWVykVZoy7oJT6n1IqHrgDDALeVEpdMxLZJ0CQMW13YKZS6qBSKgr48CHrfBH4QSn1q1IqUSl1Xil19CHTp7XdLwPvKaXClVKxRgzdHnJ6rLj17y8i3ZVS65VSB4y49hvLfsyYvhfwm1IqRCkVp5S6qpTam8KyG2H5Tccrpe4qpdYBK5JiNyxTSm0x1nUHy+9WTUTyKKWuK6V2p/E70TJBJwgtJV2UUvmSXsCrKUzjjyUZVAEKWo0TY0NUAcve/y7j83yraUoB59MQSySW6wDW8gC3bUwLlr3mZ0TEG3gG2K2UOmuMexFLkjoqIjtEpOND1nvBaH8BpVSgUirUatw5q/eFgNzALquEusYYDpY9duvpz5KyUsDJh4x/mIe1uzTwk1V8R4AELEdAtlyw/v2VUguM01h/iEiEiNzEknSSfvP0xF0cOKeUSrQadhbLUVeSc/fPwrNYEt5ZEdkgIo3TuC4tE3SC0DJNKbUBCAY+txqcKCJeWE47lRORusaGqzvgLiI9gDLAjjSs4pCxDOsjhtrGcFvxHMaywWnH/adZUEodV0r1xHJqYwKWi92+aWln8tVYvb8CxADVrTaoeY0L3AD/YNmAJrG+OJ/cOSynvlJb54MjH9JuY7ntkm30fZRSaUnQSeYBy4FSSqm8wDRAMhD3BaCUiFhvfx7h/p2F++ZRSu1QSnXG8rstJQ2nBrXM0wlCs5evgKdEpLbxeSuWu5yuYTn6WAzsB8Kx3NHUGuhsnKJJOs/vg+X0k4iIj5FgUEodw3I66gNjeFeglrHMlMwDXsdyDn9h0kAR6SMihYy91xvG4MQHZ087Y1nfAf8VkcLGekqISBtjkgVAfxGpJiK5uf9UXHLfAwNE5AnjOykhIkmn4S4BqT3zYLPdWDbmHxvXChCRQiLSOR3NBMspvWtKqTvGtZJeVuPmAk8a15s8RCRARAJTiHs7EI3lwrWniLQEngasj9DuEREvEektInmVUnHALTL5m2lppJTSL/267wWcAZ5MNqw/sDmVaaYCi433NYBjQFEbyxfALdmwllj2Gq1f663GlwHWY9lTD0u+bhvreATLRmRlsuFzsFwvicRyBNIlhflbAuEpjLvvuzCG+WC57nAKywbsCDDMavxI4CKWvecXjPZVMMYFAx9ZTdsVSzK9jeUIrI0xvLHxnV4HvjGG3VtOKu12A94yvrvbWE4HfZKetmO5jnTWmH8FMAmYYzW+OZaN/y0sRxT9jOEVsST4G8BSY1h1YAOWa0mHga5Wy0n+fXhhOWV33Vj2DqCZ2f9PcsJLjB9A0+xORHoB47DczbMay4alHpYLpHOUUnPNi07TtNToBKE5lHGa4V0se5e+WPYWpynLMw+aprkwnSA0TdM0m/RFak3TNM2mbFNDpmDBgqpMmTJOWVdUVBS+vhm5M9K16XZlPdm1bbpdzrNr164rSimbdc2yTYIoU6YMO3fudMq61q9fT8uWLZ2yLmfS7cp6smvbdLucR0RSfHBTn2LSNE3TbNIJQtM0TbNJJwhN0zTNJp0gNE3TNJt0gtA0TdNsyjZ3MWXU0j3nmbg2jAs3YiieLxcj2lSmS50Sqc+oaZqWzeXoBLF0z3lGLTlATFwCAOdvxDBqiaXDMJ0kNE3L6XL0KaaJa8PuJYckMXEJTFwblsIcmqZpOUeOThAXbsSka7imaZqriYiOoP+a/lyJuWL3ZefoBFE8X650Ddc0TXM1k/dOZvel3UzbN83uy87RCWJEm8rk8nS/b5i3hxsj2lQ2KSJN07S023d5H0uOL0GhWHpiqd2PInJ0guhSpwSfPlOTEvlyIYC7CH7e7jxZLaV+3DVN01zDxvCNDFg7AGV0352oEu1+FJGjEwRYksSWkY9zenwHQgc34np0HB8uP2R2WJqmaTYlqkSm7J3CkN+HEJ8Yf294XGKc3Y8icnyCsPZomQIMbVWBRbvCWbH/gtnhaJqm3edm7E2G/j6UqfumUiZPGTzc7n9Swd5HETpBJPPaExUJLJWP0UsOcF7fzaRpmosIuxZG0Iogtv2zjfcbvo+3uzdxiXH3TROXGMfey3vtts4c/aCcLZ7ubnwdFEj7rzfx1vy9zHupEe5uYnZYmqblYCtOrWDs1rHk8crDzDYzCSwcSI8qPRy+Xn0EYUPpAF/Gdq7B9tPXmLbhpNnhaJqWQ8UlxPHp9k8ZtWkUNQrWYP7T8wksHOi09esjiBQ8W7cEf4Rd5r+/HqNZhYLULpXP7JA0TctBIqIjeHvD2+y5vIfnqz3PG/XewNPN06kx6COIFIgIn3SpSWF/b96Yv5eo2PjUZ9I0TbOD3Zd2031Fd45eO8pnLT5jxKMjnJ4cQCeIh8qb25MvewRy5moU434+bHY4mqZlc0op5h6Zy4trX8TX05e57efSrmw70+LRCSIVjcoF8GrL8szfeY7VB/4xOxxN07KpmPgYRm0exfi/xtOsZDNCOoRQMX9FU2PS1yDS4I0nK7H5+BVGLjlA4CP5zA5H07Rs5tytc7yx/g2OXz/Oa3VeY2DNgbiJ+fvvDo1ARNqKSJiInBCRkTbG9xeRCBHZa7wGWo37TEQOicgREflGREy719TT3Y2vguoQl5DIW/P3kaiUWaFompbNbAzfSI+VPbgYdZGpT05lUK1BLpEcwIEJQkTcgclAO6Aa0FNEqtmYdL5SKtB4zTDmbQI0BWoBNYBHgcccFWtalC3oy4dPV2fbqausOR2X+gyapmkPkagSmbp3KkN/H0oJvxLM7zifpiWamh3WfRyZphoAJ5RSp5RSd4FQoHMa51WAD+AFeAOewCWHRJkOz9UvSfuaRVl8PI4D4TfNDkfTtCzqZuxNXlv3GlP2TeHp8k8zu91sSvqXNDusB4hy0OkSEekGtFVKDTQ+9wUaKqWGWk3TH/gUiACOAW8qpc4Z4z4HBgICTFJKvWdjHYOAQQBFihSpFxoa6pC2WIu8q3h/cxQ+Hm6MbZILb4/s85R1ZGQkfn5+Zodhd9m1XZB925ad23XT6yYzImZwPf46zxZ4lmZ+zTDxDDqtWrXapZSqb2uc2RepfwZClFKxIjIY+BF4XEQqAFWBpJT6q4g0V0ptsp5ZKTUdmA5Qv3591bJlS6cEfe7273y28w7rbxXk02dqOmWdzrB+/Xqc9R06U3ZtF2TftmXXdk1cMZEFlxeQxysPwU8FO/Wp6Ixw5Cmm80Apq88ljWH3KKWuKqVijY8zgHrG+67An0qpSKVUJLAaaOzAWNOlaoA7g1uUJ+Svv1lz8KLZ4Wia5uLiEi0lM2ZdnUX1gtWdXjIjoxyZIHYAFUWkrIh4AUHAcusJRKSY1cdOwBHj/d/AYyLiISKeWC5QH8GFvPVUJWqWyMvIJfu5dOuO2eFomuaiIqIjGLh2IPOOzqOVfyu+a/0dBXMVNDusNHFYglBKxQNDgbVYNu4LlFKHRGSciHQyJhtm3Mq6DxgG9DeGLwJOAgeAfcA+pdTPjoo1I7w83PgqKJDYuETeWrCXxER966umaffbc3kP3Vd058i1I3zW4jOeKfCMKSUzMsqh1yCUUquAVcmGjbF6PwoYZWO+BGCwI2Ozh/KF/BjzdDVGLTnA95tP81KLcmaHpGmaC1BKMe/oPD7f8Tkl/Esw/anpVMxfkfVn15sdWrqYfZE6ywt6tBTrwy7z2dqjNC4fQI0Sec0OSdM0E8XExzB221hWnlpJy1It+aTZJ/h7+ZsdVoa4xuN6WZiIMP6ZWhTw9eL10D3E3E0wOyRN00xy7tY5+qzqw6pTqxgaOJSvW32dZZMD6ARhF/l9vfiyeyAnI6L4eJWu+qppOZF1yYwpT05hcO3BLlMyI6OydvQupGmFggxqUY45f/7Nr4dNf+hb0zQnsVUyo1mJZmaHZRc6QdjR260rUb14Ht5dvJ/L+tZXTcv2skrJjIzSCcKOvD3c+TqoDtF343l74T5966umZWNh18IIWhHE1gtbeb/h+3zU9CN8PHzMDsuudIKwswqF/fi/jtXYdPwKM7eeMTscTdMcYMWpFfRZ1Ye7CXeZ2WYmPar0MLWekqPoBOEAvRo8wpNVizBh9VEOX7hldjiaptlJXGIc4/8az6hNo7JUyYyM0gnCAUSECc/WJG9uT14P3cOdOH3rq6ZldUklM+YemUvfan2zVMmMjNIJwkEC/Lz54rnaHL8cySerXKqMlKZp6ZS8ZMY7j76TpUpmZJROEA7UolIhXmxWllnbzvL7EX3rq6ZlNUop5h6ZywtrXiC3R27mtp9Lu7LtzA7LaXSCcLB32lamSlF/3lm0n8u39a2vmpZVxMTHMGrzKMb/NZ5mJZoR0jGEivkrmh2WU+kE4WDeHu78r2cdImPjGbFwP47qwU/TNPt5oGTG41+TxyuP2WE5nU4QTlCxiD/vd6jKhmMR/KhvfdU0l5YdS2ZkVM5stQn6NCrNE1UK88nqoxy9qG991TRXk51LZmSUThBOIiJM6FaLPD6evB6yV9/6qmkuJHnJjFntZmWrkhkZpROEExX08+bz52oRduk241cfNTscTdO4v2TGew3f46OmH5HLI5fZYbkEnSCcrGXlwgxoWobgrWf4I+yy2eFoWo6WvGRGUJWgbFkyI6N0gjDBu22rULmIPyMW7udKZKzZ4WhajpPTSmZklE4QJvDxdOebnnW4dSeOdxbpW181zZlyYsmMjNIJwiSVi/ozul0V1h29zOw/z5odjqblCNYlMyY0n5BjSmZklE4QJurXpAwtKxfi45VHOHbpttnhaFq2lbxkxpz2c2hfrr3ZYbk8nSBMJCJM7FYbP28PhoXoqq+a5ggx8TGM3jz6vpIZlfJXMjusLEEnCJMV8vdm4nO1OHrxNhPXhpkdjqZlK0klM1aeWpmjS2ZklE4QLuDxKkXo17g0328+zcZjEWaHo2nZgi6ZkXn623IRo9pXpVIRP95euI+r+tZXTcuw5CUzQjuG5viSGRmlE4SL8PF05+ugOtyMjuPdxQf0ra+algG2SmaU8i9ldlhZlk4QLqRqsTy8264Kvx25xNztf5sdjqZlKWHXwui5sqcumWFHHmYHoN1vQJMybDgWwUcrD9OoXAEqFPY3O6QcYeme80xcG8aFGzEUz5eLEW0q06VOCbPD0tJoxakVjN06Fn8vf2a2mamfirYTfQThYtzchM+71SK3lwfDQvYSG69vfXW0pXvOM2rJAc7fiEEB52/EMGrJAZbuOW92aFoqrEtmVAuoxoKnF+jkYEf6CMIFFc7jw4Rna/HSrJ188csxRrevanZIWV5cQiLXou5yNfIuV6NiuRZ1lyuRd7kWFcvMLWeISfYMSkxcAhPXhumjCBcWER3B8A3D2X15N32q9uGt+m/pp6LtTCcIF/VUtSL0afQI0zeeokXFQjSrqGvFWItPSOR6dJyx0Y/lStRdrkXGcjXqruUVGXsvIVy6GUXUmtU2l+PuJiQk2r4h4MKNGEc2QcuEPZf38Nb6t4iKi2JC8wn6qWgH0QnChb3Xvhp/nrrGWwv2svaNFuT39TI7JIdJTFTciInjatJG3tjTf3CP37LxvxETh60bvUSgQG4vCvh6EeDnRdVieSibO5aalcoS4OdNQd+kcd4E+HqRN5cnzT/7g/M2kkGAX/b9vrMqpRTzjs7j8x2fU9yvON8+9a1+KtqBdIJwYbm83Pk6KJAuk7fw7uL9fNu3XpapVZ+YqLh1J+7exv5aVOx9G3jLHv+/G/9rUXdJYUeefLk9CfD1IsDXm4qF/WhYtsC9DXyAn2W45V8v8uX2wt3t/u9o/fr1tGyZ8kZkRJvKjFpy4L7TTAJcjbzLnD/P0qdRaXt8JVomxcTHMG7bOFacWkHLki35uPnH+qloB9MJwsVVL56Xd9pU4eNVRwjdcY6eDR4xJQ6lFLdj421u7P9NAne5Yny+HnWX+BS2+Hl8PO5t4MsE+FKvdIF7G/sCvl4U9PO+9z5/bi883R17L0XSdQbru5hee7w8aw9d4v2lBwm7eJsxT1dzeBxays7dOscb69/g+PXjDAkcwqBag/RT0U6gE0QW8GKzsmw4FsGYpQf576/HiLgdm+lbMZVSRN9N4GrkXa5ExXIt8i5bwuM4vP7EfRv7a1Yb/7sJiTaX5eftcW+DXjJ/bmqXzGdzY1/Qz5v8ub3w8nC9/9hd6pR44Lt8rv4jTFhzlOkbT3EyIpIpveuSL7c+7eRsG8M3MnLTSARh8hOTaV6yudkhuYaJFSHKRq+UvoVhxHG7rEIniCzAzU14qlphNp+4wuXbljIcSbdiwr97wDF3E+6dt7fem7e1sb8SGUtsvI0N/sEwcnu53ztPXySPD9WK5aGAnxcFfb3vndsv6Gd5X8DXCx9Pd6d9F87k7iaMbl+VioX9eO+ng3SZvIUZ/errZ1OcJFEl8u2+b5m6byqVC1Tmy5Zf6qeirdlKDg8bngEOTRAi0hb4GnAHZiilxicb3x+YCCTdcD5JKTXDGPcIMAMoBSigvVLqjCPjdWXTN55+YFhMXAIjFu3j81/CuBZ1l+i7tp+Z8PZwu7dBD/DzomIRv38/W53HP35wNx2eeIxcXtlzg59Rz9UvRblCvgyevYuuk7fyv151aFm5sNlhZWvRCdG8tu41NoZvpFP5Trzf6H3XeipaKUhMAJUAifHGKyHVYX63T0C4v2XcfdMlpn+YEzgsQYiIOzAZeAoIB3aIyHKl1OFkk85XSg21sYhZwMdKqV9FxA+wfX4jh0jplsu4BMWjZQrc2/gnXcxN2uMP8PMit5d7mi5uXz/pljOTQxoO1euVLsCyoc0Y+ONOXgjewej2VXmxWdksc9NAhihltZFL2lAlOHxYWPQl/hu+jCvc4b2ARvRIyIts/tpq2nhQiffPl+Zh1hvwlIZZx5fCMJWxzVF9gF12/ZUcypFHEA2AE0qpUwAiEgp0BpIniAeISDXAQyn1K4BSKtKBcWYJxfPlsnkrZol8ufhvj0DnB5SdPOxQ/cbf9zYQJVQCi5/Jy5drw1m2aiV3TgcwqHlpvESlcwOUwY2SjWHVL1+CC9OSTZe0UbQxLD3Lz+BGMDNW+OZmbMEC+CcmMvPyFQJPL7AaK+DmAW7u//4r7qkMM95bD3P3BM9cD05377MHuLklmy+tw1JYpzHswKHD1KxV59/57psuncM+Lenw38ORCaIEcM7qczjQ0MZ0z4pIC+AY8KZS6hxQCbghIkuAssBvwEil7j+uEpFBwCCAIkWKsH79ers3wpbIyEinrStJh0cSCL4Fd63+z3q5WYbbKxYz2uUMqbWr5cNm/qrmfR9zA+8DeAOnjJcdKdxQ4oYSd5v/QtJnyzBvBbfvXL5vOh5YRq5/P7u5ozzcjGUlX35K60xtOuv12lrGv9P9O83908UBiyJ/Y130dip4laGndxduly7ORut5Xe2uJQUkGK80ivSpztUL1jc6xBuv9Jf4b/mQcfb6f2z2ReqfgRClVKyIDAZ+BB7HEldzoA7wNzAf6A98bz2zUmo6MB2gfv36qmXLlk4J2nJfvXPWlaQlUM3BBeXMaJczpNqu9Q+ZufPkFPcOt529ybebzuLv483b7apRpmCeFPb6rOZLZZiIkJ6TVtnhN7tXMiP635IZWzZuoXkWb5ctdv29dhRO8dSovdbhyARxHssF5iQl+fdiNABKqatWH2cAnxnvw4G9VqenlgKNSJYgchpbt2JqDlanT4qjGleG3FVvMGj2TtovjeerHqVpXb2oE4PL+vZc3sPb698mMi5Sl8xILzvdyvowjjxm2wFUFJGyIuIFBAHLrScQkWJWHzsBR6zmzScihYzPj5OGaxc5QUR0BP3X9OdKzBWzQ8keDv2Uqdlrl8rH8qHNqFjYj0GzdzH5jxO6s6c0UEox98hcXljzArk8cjGn/RydHFyQwxKEUioeGAqsxbLhX6CUOiQi40SkkzHZMBE5JCL7gGFYTiNhXGsYDvwuIgewVD74zlGxZiVT901l96XdTNs3zexQsr6//4QlgyGlCqC+abuVtUgeH+YPbkyn2sWZuDaMN+bv5U5cOk5M5zAx8TGM3jya8X+Np2mJpoR0DNH1lFyUQ69BKKVWAauSDRtj9X4UMCqFeX8FajkyvqwmIjqCxccXo1AsOb6El2u/TMFcusprhlw9CSE9IW9JGPgb5C6QqcVZuowNpHJRfyauDePM1Wi+61uPwnl87BRw9qBLZmQt+pfJQr7c+SWJxq2HcYlxDPt9mD6dkRFRV2FuN0vp194LM50ckogIQ1pVYFqfehy/dJtOk7ZwIPymXZadHWwM30iPlT24GHWRyU9M5uXaL+vk4OL0r5NFRERHsPrM/X0aHLh6gNfWvUZ0XLRJUWVBcTEQEgS3LkDPUAgob/dVtK1RlEUvN8HdTXju2638vO+C3deRlSSqRKbuncrQ34dSwq8EoR1DdT2lLEIniCzimz3fkJDs8Xp3cWdD+AaeX/08FyJz9kYoTRIT4aeXIXwHPDMdSjVw2KqqFc/DsqFNqVE8L6+F7OHLX8JITKmeeTZ2M/Ymr617jSn7ptCxXEdmtZul6yllITpBZBGbz29+YFiCSqCkX0kuRF4gaEUQOy7uMCGyLOS3D+DwUmj9H6jW2eGrK+jnzdyXGvJcvZJ8s+4Er87dTfTdeIev11WEXQuj58qebD2/ldENR/Nxs49dq56SliqzH5TT0iA2IRalFI2LNWZ66+kPjD9z8wyvrXuNQb8MYmSDkfSo0sOEKF3cjhmw9Rt49CVobKv0l2N4e7jzWbdaVC7qzyerjtBtajTf9atPiXzZe0O54tQKxm4di7+XPzPbziSwcKDZIWkZoI8gsoCVp1Zy9c5V+tfob3N8mbxlmNdhHk1KNOGj7R8xbts44hLinBukCytwdSesGgGV2kLb8ZaL004kIgxsXo7v+z/KuWvRdJ60mV1nrzk1BmeJS4xj/F/jGbVpFNUCqrHg6QU6OWRh+gjCxSWqRIIPBVOlQBUaF2uc4nT+Xv580+obJu2dxIwDMzh54yRftvySgFwBTozWBV3YQ/VDE6FoLej2A7ib9yffqnJhfhrShBd/3EnP6dv55JmadKvn+IJrznKvZMblf0tmeKb0jIkTxMXFER4ezp07d0yLIbm8efNy5MiR1Cd0AB8fH0qWLImnZ9p/E50gXNzG8I2cvnmaT5t/mmppaXc3d16v+zqV8ldizJYxBK0M4utWX1MtoJqTonUxN87BvB7Eefrj3ms+ePmaHREVCvuz9NWmDJm3m+EL93H80m3eaVvlgX60sxpXLJkRHh6Ov78/ZcqUcZmy7Ldv38bf3/kdTimluHr1KuHh4ZQtWzbN8+lTTC5u5sGZFPUtSpsybdI8T7uy7fix3Y8A9Fvdj9WnV6cyRzYUcwPmPgdxd9hfawz4u06NpPy+Xvz4QgP6NirNtxtP8dKsndy+kzVPCVqXzPDx8HGpkhl37twhICDAZZKDmUSEgICAdB9N6QThwvZH7Gf35d30rdo33Yfq1QKqEdohlGoB1Xhn4zt8tesrEhJzSPmH+LuwoC9cPQE9ZhPt+4jZET3A092N/3SpwX86V2fDsQiembKVv69mredZkpfMCO0Y6nIlM3Ry+FdGvgudIFxY8KFg/D39ebbSsxmaPyBXADNaz+C5Ss/x/cHveW3da9y+e9vOUboYpeDn1+H0Ruj0Pyj3mNkRPVTfxmWY/UIDLt+OpfPkzWw7eTX1mVzAudvn6LuqLytPrWRI4BC+efwb8njlMTsszc50gnBRf9/6m9/O/kb3yt3x9cz4uXNPd0/GNB7D/zX6P7Zd2Eavlb04c/OM/QJ1NRs+g33zoOUoCOxpdjRp0qRCQZYNaUoBXy/6fr+dedv/Njukh9oYvpEeK3rwT9Q/umRGGly6dIlevXpRrlw5WrRoQePGjfnpp59Yv349IsKMGTPuTbt3715EhM8//xyA/v37U7ZsWQIDAwkMDOSbb75xauz6V3VRsw7PwsPNg95Ve9tled0rd+e71t9xM/YmvVb2YlP4Jrss16XsDYH1n0DtXvDYu2ZHky5lCvry05CmNK1QkNE/HeDD5YeIT3CtbtitS2YU9y2e7UpmLN1znqbj11F25Eqajl/H0j3nU58pFUopunTpQosWLTh16hQbN24kNDSU8PBwAGrUqMGCBf92qxoSEkLt2rXvW8bEiRPZu3cve/fuZdiwYZmOKT10gnBB1+5cY+mJpXQs15FCuQulPkMa1S9an9COoRT3K86Q34cw8+DM7FPs79QGWD4UyraAp792+rMO9pDHx5Mf+j/KwGZlCd56hv4zd3Az2jUuXicvmTG7/exsVTJj6Z7zjFpygPM3YlDA+RsxjFpyINNJYt26dXh5efHyyy/fG1a6dGlee+21e+/v3LnDpUuXUEqxZs0a2rVrl6l12lOabnMVy9WNBlj6mQZLz3B/qWyzdXEtoUdDiU2IpX/1/nZfdnG/4sxqN4sxW8fw5a4vOXrtKGObjMXHIwuXpb58BOb3hYCK0H02eHilPo+LcncT3u9YjUpF/XnvpwN0mbKFGf3qU76Qn2kxhV0L4831b/JP5D+MbjiaoMpBWe7i79ifD3H4wq0Ux+/5+wZ3kx2xxcQl8M6i/YT8ZfuUX7Xiefjg6eoPXe+hQ4eoW7fuQ6fp1q0bCxcupE6dOtStWxdvb+/7xo8YMYKPPvoIgNmzZ1OzZk1bi3GIVI8gRKQ1cBz4EGhvvMYCx41xmh3FxMcQcjSEx0o+Rrl85RyyjtyeuZnYYiLD6gxj9enV9FvTj4tRFx2yLoe7fclyO6unD/ReALnymR2RXXSvX4p5LzXiVkwcXSZvYcOxCFPiWHlqJX1W9eFO/B1mtp1Jzyo9s1xySIvkySG14Rn11ltvUbt2bR599NF7w7p3787ChQsJCQmhZ88Hr5tZn2JyZnKAtB1BfA08qZQ6Yz1QRMpi6QyoqgPiyrGWnVjGjdgbDjl6sCYivFTrJSrmr8jITSMJWhHE83mfpyUtHbpeu4qNhHndIfoaDFgF+VzvdtbMeLRMAZYOacpLs3YyYOZfvN+hGgOaOuehr7jEOL7Y+QVzj8ylbuG6fNHyiyzdOVVqe/pNx6/j/I2YB4aXyJeL+YNTrmCQmurVq7N48eJ7n7/88ktiY2OpX7/+vWFFixbF09OTX3/9la+//pqtW7dmeH32lpZrEB5AuI3h5wHznqPPhhISE5h1eBY1C9akXpF6Tllny1Itmdt+Lr6evnxz6RsWH1uc+kyuIDEBFr8IF/dbSmgUDzQ7IocoVSA3i19pwhNVizBuxWFGLTnA3XjHXryOiI5g4NqBzD0ylz5V+zCjzYwsnRzSYkSbyuTydL9vWC5Pd0a0qZyp5T7++OPcuXOHqVOn3hsWHf3g8y7jxo1jwoQJuLu7PzDOTGk5gvgB2CEiocA5Y1gpIAj43lGB5US///07526f4426bzj1ML58vvLM6zCPgUsH8uG2Dzl67SjvNHjH1Do6D6UUrH4Xjq2B9p9D5bZmR+RQvt4efNunHl/+eoxJf5zg1JUopvWpRwFf+19rccWSGc7QpY7l8urEtWFcuBFD8Xy5GNGm8r3hGSUiLF26lDfffJPPPvuMAgUKkCdPHiZMmHDfdE2aNMnUehwl1QShlPpURJYCnYGkY63zQG+l1GEHxpajKKUIPhRMKf9SPPHIE05ff17vvLxc+GX2+O3hx8M/cvLmSb547Avy++R3eiyp2jYZdnwHTV6DBi+ZHY1TuLkJw9tUpmIRP0Ys2k+nSZuZ0a9+6jOmkVKKkKMhTNwxkWJ+xZj21DSXeyra0brUKZHphGBLsWLFCA0NBR6sxdSyZcsHpv/www/vvQ8ODrZ7POmRpruYlFJHAHNKEOYQuy7t4sCVA7zX8D3c3cw5zHQXd4Y/OpzKBSrz4dYP6bmyJ1+3+prKBTJ3mG1Xh5fBL+9bOvx5cpzZ0Thd58ASlA7wZdCsnTw7ZSsDa3hk+qpRTHwM47aNY8WpFTxW8jE+af6JfipaAzL5HISI5MAqcI4RfCiY/N756VzB8T2dpebp8k/zY7sfiUuIo+/qvvx69lezQ7I49xcsGQQlH4Wu34JbznyMJ7BUPpYPbUb5wn58szuWqetPZvh5FuuSGa8GvqpLZmj3ScttrnVTeNUDAh0fYvZ38sZJNoRvIKhKkMt0yVijYA1CO4ZSMX9F3lr/FpP2TCJRmfhk79WTEBIEeYpDz1DwdI3vySxF8/owf1BjHi3qzoQ1R3lrwT7uxKWvGGNSyYwLUReY/MRkXqn9ii6Zod0nLaeYdgAbAFtXTfPZNZocatbhWXi7exNUJcjsUO5TKHchZraZyX/+/A/f7v+WY9eP8WnzTzNVGypDoq9ZnnVQCnovAt8c3gmSIZeXO6/U9qZ5zZJ88esxTl+JYnrfehTO8/CHHhNVIt/u+5ap+6ZSKX8l/tvqv9nqqWjNftKyu3AEGKyUapX8BVxxcHzZXkR0BD+f/JnO5TtTwKeA2eE8wMvdi3FNxjGywUg2hm+kz6o+nLt1LvUZ7SXuDoT0hJvh0DMEAso7b91ZgIjw2hMVmdanLmEXb9N58hYOnr+Z4vS37t5i2Lph2bZkhmZfaUkQHz5kutfsF0rONO/oPOIT43m++vNmh5IiEaF31d5Me2oaETERBK0MYtuFbY5fcWIiLH0Fzv0JXafBI40cv84sqm2NYix6pTECdJu2lZX7/3lgmrBrYQStCGLL+S2Mbjiaj5t97DKnNDXXlGqCUEotUkqFpTBuqd0jykGi4qKYHzafJx55gtJ5SpsdTqoaFWtESIcQCucuzMu/vczsw7MdW+zv97FwaAk8ORZqPOO49WQT1YvnZdnQZlQvnpch83bz1W/HSEy0/D7WJTN+aPtDti2Z4WpEhD59+tz7HB8fT6FChejYsSNguY116NChD8xXpkwZatasSa1atWjdujUXL5pTCifNfVKLSDEsD8eVAy4D85VSxxwVWE6w5PgSbt+9Tf8a/c0OJc1K+ZdiTvs5vLf5PT7b8RlHrx1lTOMxeLt7pz5zeuz8AbZ8BfVfgKav23fZ2Vghf2/mvdSQ0UsO8tVvxzl68Tolyv/OgmMh2aJkhsNMrAhRlx8c7lsYRhzP8GJ9fX05ePAgMTEx5MqVi3Xr1lGiRNqetfjjjz8oWLAgo0eP5pNPPnF6XxCQxttcRWQYEAycBCZjuWj9mYg8JaJve8iIuMQ4Zh+eTd3CdaldqHbqM7gQX09fvmz5Ja/UfoXlJ5fzwpoXuBxt4z9XRh3/FVYOh4qtod3ELFm620zeHu58/lwtXm9dmI23P2LBsRC6lAvKESUzMsxWcnjY8HRo3749K1euBGDRokU2C/I9TIsWLThx4kSm48iIVI8gRKQD0AhoCzyHpew3wGpgFFBWRC4opVY4LMps6Jczv/BP1D+MajDK7FAyxE3ceDXwVSrlr8TozaMJWhHEV62+olahWplb8D/7YEE/KFIdus0E9zQf5GpW9kXs4+cr75LL7zax//Rm9bn6dC0dSd1HXPDJeGdYPRIuHsjYvDM72B5etCa0G5/q7EFBQYwbN46OHTty6NAhBg8ezKZNae+wa8WKFU6v4pokLXv/w4C3jb4f6gNdgNxAa2A7sMSYRkujpLIaZfOW5bFSrt1ncmqeLP0kc9rPwcvdiwFrBrD85PKML+xmOMzrAbnyQ68F4G1eHwhZlVKKeUfmMWDNAHw8fAjpOJcl/YaQ28udoOl/smS3rbqbmiPVqlWLM2fOEBISQuvWae8hoVWrVgQGBnLr1i1GjTJnRzItu2eFlVJJt0Q0AZorpZSIfAtsUkqNEpEijgsx+/nznz85eu0oHzb+MFs8mFQpfyVCO4QyfMNw3tv8HkevHeWtem/h4ZaOvf87Ny3POtyNghfWQJ5ijgs4m3pYyYxlQ5ryytxdvLVgH8cuRfJOm8q4ueWgU3ep7el/mDflcQNWZnr1nTp1Yvjw4axcuZI7d+6kaZ6kaxBmSsvWKVJEkqK8CXQUES+gI3BbRHyBSEcFmB0FHwomwCeAjuU7mh2K3eTzycfUp6bSu2pvZh+ezau/vcrN2JTvx79PQpzltNKVY9B9luX0kpYuqZXMyO/rxewXG9Kr4SNM23CSQbN3Ehkbb2LEOcsLL7zABx98QPXqWetvOy0JIhgYbbzvB7QClhr/9gPeAkIcEFu2FHYtjK0XttK7am/73/ljMk83T0Y2GMm4JuPYcWkHPVf25MT1VC6uKQUr3oBTf1j6ki7fyimxZidpLZnh6e7Gx11qMK5zdf4Ii+DZKVs5d+3BvglyJN/C6RueTiVLlmTYMNtn4oODgylZsuS9V3i465wGTGt/EHNF5BPgE6XUWwAikht4F6gBdHNciNlL8KFgcnnkonvl7maH4jBdK3albN6yvPHHG/Re1ZvxzcfT6pEUNvwbP4c9c6DFO1Cnj+1pNJsSVSKrb6xm9e+r01wyQ0R4vnEZyhX049W5u+g0aTPT+tSjYbkcXr4kE7eyPkxk5IMnV1q2bHmvzHf//v3p37//A9OcOXPGIfGkV1oelFNKqV5AGLBMRNaLyDrgZyw9zXVTDn1aKvu4GHWRNafX8GzFZ8nr/ZBzntlAYOFAQjuGUjZvWYb9MYxv93374EN1++bDHx9BrSBoNdr2gjSbkkpmrLq5ig7lOqS7ZEazigVZNrQZ+X296D1jO6F//e3AaLWsKs1XSJVSPyqlnlBKtVRKPW68/04nh7SbfXg2CkXfan3NDsUpivoWJbhtMB3LdWTS3km8veFtouOMUxqnN8GyIVCmOXT6n37WIR2sS2Z0y9+NT5p9kqGSGWUL+vLTq01pUqEgI5ccYOzPh4hPMLFir+Zy0vqgnLvVhWpExEtEXhKRh3YiJCJtRSRMRE6IyEgb4/uLSISI7DVeA5ONzyMi4SIyKa0NclW37t5i0bFFtC7TmuJ+xc0Ox2l8PHz4pNknvF3vbX7/+3f6ru7L+bObYH5vKFAOeswGD/t3nZldJS+Z8ViexzJVMiNvLk9+6FefF5qWZeaWMwwI3sHN6Dg7RqxlZWnpDyIIuAbsF5ENItIaOAW0B3o/ZD53LE9dtwOqAT1FpJqNSecrpQKN14xk4/4DbExbU1zbwrCFRMdHM6D6ALNDcToRoX+N/kx+YjL/RJ4naN2r7PDJBb0XWp550FIVlxjH+L/GM3LTSKoFVGPB0wuoU7iOXZbt4e7GmKerMeHZmvx56ipdp2zhVIS+MVFL2xHE+0A9pVRx4E0s1x5eUUp1VUrtfsh8DYATSqlTSqm7QCiWfq3TxOiQqAjwS1rncVV3E+4y98hcGhZrSNWAqmaHY5pmheowL8qT/AnxDCqQi9BL2xxb7C+buBJzhYFrBzL3yFz6VO3jsJIZPR59hLkDG3EjJo4uk7ew6XiE3dehZS1pSRB3lVInAIyEcFwp9XMa5isBWHccEG4MS+5ZEdkvIotEpBSAUd/pC2B4Gtbj8ladXkVETESOPHq4JzEBFg+kzIWDzGv0MU1LNOPj7R8zdttY4hL0KY2U7L28l+4/d+fw1cOMbz6edxu8i6ebp8PW16BsAZYNaUqxvLnoP3MHP249o5N4Diap/fgiEg58aTXoLevPSqkvH5jJMl83oK1SaqDxuS/QUCk11GqaACBSKRUrIoOBHkqpx0VkKJBbKfWZiPQH6lvPZzX/IGAQQJEiReqFhoampc2ZFhkZiZ9f2spAKKX49J9PEYSRxUa6dInl9LQrvSoc/46S51dwvMIgzpfsQKJKZOWNlfxy6xfKeZfjxUIvksfdMX0hO7JdjqKUYuPtjSy5voQCHgUYWGggJbwe3L9yVNti4hXT98ey53ICLUt60KeaFx5OfPLaHu3KmzcvFSpUsFNE9pGQkIC7u3u65zt79iw1a9Zk+PDhjBkzBoCrV69SsWJFBgwYwBdffMEnn3zCjz/+eO/p6yeffJKxY8fet5wTJ05w8+b9D7C2atVql1Kqvs0VK6Ue+gI+eNjrIfM1BtZafR4FjHrI9O7ATeP9XOBv4AyWXutuAeMfFme9evWUs/zxxx9pnnbDuQ2qRnANtfzEcscFZCfpaVe6bJ2s1Ad5lFo96oFRq0+tVvVn11dPLHhCHbxy0CGrd1i7HCQ6LlqN3DhS1QiuoYb8NkTdjL2Z4rSObFtCQqIav/qIKv3uCtV92lZ1NTLWYetKzh7tOnz4cLrnuRx1WfVb3U9FREdkev223Lp1K0PznT59WpUtW1YFBgbeGzZlyhRVu3ZtNWTIEKWUUh988IGaOHHiQ5dj6zsBdqoUtqtpeQ5i7MNeD5l1B1BRRMoapTmCgPsquRl9TCTphKV7U5RSvZVSjyilymA5zTRLKfXAXVBZQfChYIrkLkLbsm3NDsUcR36GtaOh6tPQ+qMHRrct25ZZ7WbhJm70W92PVadWmRCk60itZIYzubkJ77atwn971GbPuRt0nryZY5dumxKLs0zbP43dl3Yzbd80uyzvzJkzVK1alZdeeonq1avTuXNnYmJiOHnyJG3btqVevXo0b96co0ePApYH5xYtWnRvfuujqNy5c1O1alV27twJwPz58+ne3bEP3Kal3PdDe6lQStl8flwpFW+cKlqL5ejgB6XUIREZhyVjLQeGiUgnIB7LnVL90xm/Szt05RA7Lu5geP3hDj1v7LLCd8LigVCiHnSdDm6290eqBlQlpEMIb61/i3c3vUvY9TCG1RmGu1v6D8Wzsk3hm3h307sATH5iMs1LNjc5IouudUpSJsCXQbN38cyUrXwdFMgTVbNWfc4Jf03g6LWjD53mbsJdDlw5gEKxIGwBR68exdM95f+3VQpU4d0G76a67uPHjxMSEsJ3333HM888w+LFi5k5cybTpk2jYsWKbN++nVdffZV169aluqygoCBCQ0MpUqQI7u7uFC9enAsXLtwb/9///pc5c+ZY2jxhAm3atEl1mQ+TllIbu6zej8VyailNlFKrgFXJho2xej8Ky6mnhy0jGEs9qCxn5qGZ+Hn68WzFZ80OxfmunbaU7vYvCj1DwSv3QycPyBXAjNYzGP/XeH44+APHrh9jQosJpu09O1OiSuTb/d8yde9US8mMlv+lVJ60PxXtDHUeyc/yoU15adZOBs7ayci2VRjUopxLX1NLr3+i7u/H+0LUBbt0BVy2bFkCAwMBCAwM5MyZM2zdupXnnnvu3jSxsbFpWlbbtm35v//7P4oUKUKPHj0eGP/mm28yfLj97u1JNUEopX5Mei8ib1h/1lJ27vY5fj37K/2q9cPPK2tdIM206GuW0t0qAXovAr9CaZrN092T/2v8f1QuUJlPt39K75W9+ebxbyibt6yDAzbPrbu3GL1pNBvCN9CxXEfGNB6ToaeinaFY3lwsHNyE4Qv38enqo4Rdus0nXWvi4+n6R3qp7elHREfQbkk7FJabdhSKW3dvMfGxiZm+pdjb+9+inO7u7ly9epV8+fKxd+/eB6b18PAgMdHyNHtiYiJ37969b7yXlxf16tXjiy++4PDhwyxfnon+V9IgvZ0R6Pvd0mj24dm4iRu9q6b4LGH2FB8Lob3hxlkImgcFK6Z7Ed0rd+e71t9x6+4teq3sxcbwbPGs5AOsS2aMajAqwyUznCmXlzuTetXhzScrsWT3eXp99yeXb6etfwNXNm3/NBLV/WVGElWi3a5FWMuTJw9ly5Zl4cKFgOVGoX379gFQpkwZdu2ynLRZvnw5cXEP3gL+9ttvM2HCBAoUKGD32JLL+r3VuKAbd26w9MRS2pdtTxHfrHWuNlMSE2Hpq/D3VugyFUo3yfCi6hetT0iHEEr6l2To70P54eAP2ep+/OQlM3pV7ZVlTteICK8/WZEpvety+J9bdJm0hYPn09j3h4vad3kfcYn3b4zjEuPYe3mvQ9Y3d+5cvv/+e2rXrk316tVZtmwZAC+99BIbNmygdu3abNu2DV9f3wfmrV69Ov369XNIXA9I6fYm9e/tp7ex3GZ6C8vF5KT3t4Fbqc3vrJcr3eY6de9UVSO4hjp27ZhzArKTTN9a+NtYy+2sG7+wSzxKKRV1N0q9vf5tVSO4hhqxYYSKjotO9zJc6TbXuwl31fjt41WN4Brq+VXPZ/p2SrPbdiD8hmr0yW+qyvur1ar9F+y2XLNuc3W0jN7mai+OuM3VXymVx3h5WL33V0pl/yuI6XQn/g4hR0NoVqIZFfOn//RKlrUrGDZ9AXX7QbM37bbY3J65mdhiIq/XfZ01p9fQb3U/LkZdtNvynSmpZMacI3McWjLDmWqUyMuyoU2pUsyfV+bu5pvfj2erI72cTp9isrPlJ5dz7c61nFVW48RvsOItqPAkdPjS7qW7RYSBNQfyv8f/x9+3/6bHih7subzHrutwNGeXzHCmwv4+hLzUiGfqlODLX48xNGQPMXcTzA5LswOdIOwoITGBWYdnUS2gGo8WfdTscJzj4gFLf9KFq8FzweCeljunM+axUo8xr/08/L38eWHtCyw6tij1mUymlGLekXkMWDMAHw8f5rSfQ4dyHcwOy+58PN35onttRrWrwqoD/9D9221cvGn+xWt9NPOvjHwXOkHY0fpz6zl76ywDqg/IMhccM+XmeZjbHbzzQO8F4O3v8FWWy1eOue3n0rBoQ8ZuG8vHf378wMVFVxETH8PozaP59K9PaVKiCaEdQ6lcoLLZYTmMiDD4sfLMeL4+pyIi6TRpM3vP3TAtHh8fH65evaqTBJbkcPXqVXx8fNI1n+N293KgmYdmUsKvBE+WftLsUBzvzi2Y1x1ib8MLayCP8zpByuudl8lPTObr3V8z89BMTtw4wRctv6CAj+Nv+0urc7fP8eYfb3Ls+jFeDXyVwbUG4yY5Y3/siapFWPJqUwbO2kH3b7cxsVstOgfaKuTsWCVLliQ8PJyICNcpW37nzp10b6TtxcfHh5IlS6ZrHp0g7GTP5T3si9jHyAYj8XDL5l9rQhws7A+Xj1g6/Slaw+khuLu581b9t6iYvyIfbv2Qnit68s3j37jEHvqm8E2M3DQShWLSE5NoUbKF2SE5XeWi/iwb0oxX5uzi9dC9hF28zfDWlXFzYkVYT09PypZ1rYcs169fT5069unoyRlyxi6NE8w8OJO83nnpWqGr2aE4llKw8i04+Ts8/RVUeMLUcJ4u/zQ/tvuReBVP39V9+eWMef1LJapEpu6bypDfh1DMtxjzO8zPkckhSQFfL2a/2JCeDUoxZf1JBs/ZRWRsvNlhaemgE4QdnL55mvXn1tOjcg9yez685lCWt/lL2D0Lmg+Hus+bHQ0ANQrWILRDKJXyV+LtDW/zvz3/e+CpWEe7dfcWw9YNY8reKXQo14HZ7We7XD0lM3h5uPFJ15p8+HQ1fj9yiW5Tt3LuWrTZYWlppBOEHcw6PAtPN096VulpdiiOtX8h/D4Oaj4Hj79vdjT3KZS7ED+0+YGuFboyff90Xv/jdSLvOqdf5axYMsOZRIT+TcsSPKAB52/E0HnyFnacuWZ2WFoa6ASRSVdirrD8xHI6VeiU5R96eqgzW2DZq1C6KXSebPdnHezBy92LsU3GMqrBKDaFb6LPqj78fetvh64zK5fMcLYWlQqxdEhT8uXypNd3fzJ/h2N/Gy3zdILIpJCjIcQlxtGvmpNqo5gh4hiE9oJ8paHHHPDwTn0ek4gIvar24tunvuXKnSv0XNmTozEP7wcgI+IS45jw1wRGbhpJtYBqzO84nzqFs87FR7OUL+THT682pVG5AN5dfID/rDhMfIJzTwdqaacTRCZEx0UzP2w+rUq1okzeMmaH4xiRETC3G7h7Qp9FkNt1biV9mIbFGhLSIYQivkWYcnkKsw7Nstv98LZKZhTKnbaS5hrkze3JzP6P0r9JGb7ffJoXf9zJrTuu+SxLTqcTRCb8dOInbsbeZECNbFpW4240hPSAyMvQcz7kL2N2ROlSyr8Uc9rNoWaumkzcOZH3t7xPbELaOmZJSXYumeFMHu5ufNipOp90rcmWE1foOnkLp69EmR2WloxOEBkUnxjP7MOzCSwUSGDhQLPDsb/EBFjyEpzfDc/OgJL1zI4oQ3J75ubFQi/yauCrLD+5nAFrBnA5+nK6l5NTSmY4W6+GjzBnYEOuRd2ly+QtbDlxxeyQNCs6QWTQb2d/43zkefrX6G92KI7xy/twdAW0/RSqdjQ7mkxxEzdeqf0KX7X8ihM3ThC0Ioh9EfvSPH9MfAzvbX7vXsmMkA4hLvFAXnbRqFwAy4Y0o0geb57/4S9mbTtjdkiaQSeIDFBKMfPQTMrkKUOrUq3MDsf+/pwGf06Bhq9Ao1fMjsZunij9BHPaz8HL3YsBawaw7MSyVOc5d/scfVf1ZcWpFbwa+Cr/e/x/5PXO64Roc5ZHAnKz+JUmtKxUiDHLDjHrUCxx+uK16XSCyIDjscc5fPUwz1d/PvvV1zm6EtaMhCodoc3HZkdjd5XyVyK0Qyh1C9fl/S3vM+GvCcQn2n66d1P4JoJWBHEh6gKTnpjEK7VfyX6/twvx9/Fk+vP1efmx8qw7F8/z3//F9ai7qc+oOYz+a8+A32/+TgGfAjxd7mmzQ7Er/1vHYdGLULwOPPMduLl+Z/QZkc8nH9Oemkafqn2Yc2QOr/z2CjdjbxIRHUH/Nf25HH35XsmMor5Fc3zJDGdydxNGtqvCSzW92HX2Ol2mbOH4pdtmh5Vj6QSRTsevH+fwncP0rNITHw9zqjI6xPUz1DzwH/ArDL3mg1f2Lhni4ebBuw3eZVyTcey6tIugFUF8tuMzdl/aTd9Vfe+VzJjTfo4umWGCpiU8CRnUiKjYBLpO2cofR9N/Y4GWeTpBpFPwoWC8xIugykFmh2I/Mddh7nOIiofeiyxJIofoWrErP7T5gej4aNacWYNCcSHqAq8FvqZLZpisXun8LB/alNIBuXnhxx18t/GU7tvByXSCSIdLUZdYdXoVjfwakc8nn9nh2Ed8LIT2getnOFhjNBSqZHZEThdYOJCmxZve++whHlyOuaxLZriA4vlysfDlxrStXpSPVx1hxKL9xMbr7kydRSeIdJh7ZC6JKpFW/tnkziWlYNlQOLsZOk/hZj7n9+vgCiKiI/jl7L9lwuNVPEtPLOVKjL4n3xXk9vJgcq+6vP5ERRbtCqfXd9u5Epm5Bx61tNEJIo0i70ay8NhCnir9FAU9s0lRvj8+hgMLLJVZaz1ndjSmmbZ/2gPlwRNVItP2TTMpIi05NzfhzacqMblXXQ5duEnnSVs4fOGW2WFlezpBpNGiY4uIjItkQPVsUlZj92zYOBHq9LX07ZCD7bu874F+reMS49h7ea85AWkp6lCrGItebkJCouLZqVtZc/Ci2SFla9m8b0z7iEuIY/aR2Txa9FGqF6zOetabHVLmnFwHK96A8o9Dx/+6ZOluZ1rUaZHZIWjpUKNEXpYPbcqg2bt4ec4uhreuxJBWFfQ1IwfQRxBpsObMGi5HX6Z/9f5mh5J5lw7B/OehYGV47kdLlVZNy2IK5/EhdFAjugQW5/NfjjEsdC934vTFa3vTRxCpSCqrUSFfBZqXaG52OJlz6wLMfQ68/aD3QvDJY3ZEmpZhPp7u/LdHIJWK+jNxbRhnr0YxvW99iubNRs8nmUwfQaRi64WtHL9+nH7V+2XtQ9jY2zCvO9y5Cb0WQN4SZkekaZkmIrzasgLT+9bn5OVIOk3azL5zN8wOK9vQCSIVMw/NpHCuwnQom4VLOyfEw8L+cOmw5bRSsVpmR6RpdvVUtSIsfrUJXh5udP92G8v2njc7pGxBJ4iHOHz1MNv/2U7var3xzKrn6pWCVW/Did+gwxdQ8UmzI9I0h6hSNA/LhjSldsl8vB66ly9+CSMxUT95nRk6QTxE8KFgfD19ea5SFn5GYMtXsCsYmr0J9bPJLbqaloIAP2/mDGxIj/ql+N+6E7wydxdRsbar9Wqp0wkiBRciL/DLmV/oVrEb/l7+ZoeTMQcXw28fQo1n4fExZkejaU7h5eHG+GdrMqZjNX49fIlnp24l/Hq02WFlSTpBpGD24dkIQp9qfcwOJWPOboOfXoZHGkPnKeCmf2ot5xARXmhWlpkDGnD+RgydJ21h55lrZoeV5Th0qyEibUUkTEROiMhIG+P7i0iEiOw1XgON4YEisk1EDonIfhHp4cg4k7sZe5PFxxfTrmw7ivoWdeaq7ePKCQjtCfkegaB54Klv+9NypscqFeKnV5vi7+NBz+/+ZOHOc2aHlKU4LEGIiDswGWgHVAN6ikg1G5POV0oFGq8ZxrBo4HmlVHWgLfCViORzVKzJLQhbQEx8DP2q93PWKu0n6grMfRbE3fKsQ+4CZkekaaaqUNiPpUOa0qBsAUYs2s/HKw+ToC9ep4kjjyAaACeUUqeUUneBUKBzWmZUSh1TSh033l8ALgOFHBapldiEWOYemUvT4k2zXsf0cTEQEgS3L0LPUChQzuyINM0l5MvtRfCABvRrXJrvNp1m4I87uHUnLvUZczhHJogSgPXxXLgxLLlnjdNIi0Tkga67RKQB4AWcdEyY91txcgVX71ylf43+zlid/SQmwpJBEL7T0l1oqUfNjkjTXIqnuxtjO9fg46412HT8Cs9M2cqZK1Fmh+XSxFE9NIlIN6CtUirpukJfoKFSaqjVNAFApFIqVkQGAz2UUo9bjS8GrAf6KaX+tLGOQcAggCJFitQLDQ3NVMyJKpGPL3yMl5sX7xR9J8UnpyMjI/Hz88vUuuyt/ImZlApfyonyLxBeKk0Hag9wxXbZQ3ZtF2Tftjm6XUeuJjBp7x0Ahgb6UDXAOf2vu+Lv1apVq11Kqfo2RyqlHPICGgNrrT6PAkY9ZHp34KbV5zzAbqBbWtZXr149lVnrzq5TNYJrqJUnVz50uj/++CPT67Kr7dOV+iCPUiuHK5WYmOHFuFy77CS7tkup7Ns2Z7TrzJVI9eQX61X5USvV7G1nHL4+pVzz9wJ2qhS2q448xbQDqCgiZUXECwgClltPYBwhJOkEHDGGewE/AbOUUk6rxRx8KJhivsVoXaa1s1aZeWGrYfU7UKkdtB2f40t3a1palQ7wZcmrTWhRqRDvLz3ImGUHiUtITH3GHMRhCUIpFQ8MBdZi2fAvUEodEpFxItLJmGyYcSvrPmAY0N8Y3h1oAfS3ugU20FGxAuyL2Mfuy7vpW60vHm5ZpMjt+d2w6AUoVhu6fQ9uzjlM1rTswt/Hk++er8/gFuWYte0s/Wf+xY3ou2aH5TIcuiVUSq0CViUbNsbq/Sgsp56SzzcHmOPI2JILPhiMv5c/z1Z81pmrzbgbf8O8HpC7IPScD16+ZkekaVmSu5swqn1VKhbxZ/SSA3SZvIUZ/epToXAWraBgR1lkV9mx9l7ey29//0avKr3I7Znb7HBsm1gRoi4/ODxXAfAv4vx4NC2b6VavJGUL5mbw7F10nbyV//WqQ8vKhc0Oy1S6/gLw4dYPAYiKc+Fb3mwlB4AYXT5A0+ylXukCLBvajJIFcvNC8A6+33w66aaZHCnHJ4jj149z8qblEYs1Z9ZwJeaKyRFpmmamEvlysejlxrSuVpT/rDjMu4v3ExufM7szzfEJYvbh2bgZX0OiSmTavmkmR6Rpmtl8vT2Y0rsuwx6vwIKd4fSZsZ0rkbFmh+V0OTpBRERHsOr0KhKx3NoWlxjH0hNLXesoIiEOfh9ndhSaluO4uQlvta7M/3rWYX/4TTpP2sKRf26ZHZZT5egEMW3/NBLV/fc9u9RRxPWzMLM9bPrC7Eg0Lcd6unZxFr7cmPjERJ6dupVfDl00OySnydEJYt/lfcQl3l+wKy4xjr2X95oTkLVDP8G05hBxFLr9AL4p3E2R0nBN0+ymVsl8LB/ajIqF/Rg8ZxeT/ziRIy5e5+jbXBd1ctpD2ml3NxrWjITdP0KJ+pYH4PKXsfQKp2maaYrk8WH+4Ma8u3g/E9eGcezSbSY8Wwsfz+z7gGqOThAu59IhWDgArhyz9CHd6j1w9zQ7Kk3TDD6e7nzVI5BKRfyZuDaMM1ej+a5vPQrnyZ6dcuXoU0wuQyn46zuY3gru3IC+P8GTH+rkoGkuSEQY0qoC3/atx/FLt+k0aQsHwm+aHZZD6ARhtuhrML8PrBoOZVvAy1ugfCuzo9I0LRVtqhdl8StNcHcTnvt2Kyv2XzA7JLvTCcJMZ7daLkQfWwutP4ZeC8DPKR3naZpmB1WL5WHZ0KbULJGXofP28OUvYSRmo+5MdYIwQ2ICrJ8AwR3Awwte/AWaDAU3/XNoWlZT0M+bOQMb8ly9knyz7gSvzt1N9N14s8OyC71Fcrab5+HHp2H9J1DzORi8EUrUNTsqTdMywdvDnc+61eL9DlX55fBFuk3dxvkbMWaHlWk6QTjT0ZUwrSlc2Atdv4VnpoO3LimsadmBiDCweTm+7/8o565F03nSFnadvW52WJmiE4QzxN2BVSMgtBfkewRe3gS1g8yOStM0B2hVuTA/DWmCn7c7Paf/yaJd4WaHlGE6QThaRBjMeAL+mg6Nh8KLv0JAebOj0jTNgSoU9mfpkKbUL5Of4Qv38emqIyRkwYvX+kE5R1EK9syG1e+CZy7otRAqZaG+rjVNy5R8ub348YUGjPv5MN9uPMXxy5E8VzJrJQmdIBzhzk34+Q04tMTybEPX6ZCnmNlRaZrmZJ7ubvynSw0qFfHjw58PczQcqteJ5pEAF+25Mhl9isnewndanm04vAyeGAN9l+rkoGk5XN/GZZj9QgNuxio6T97MtpNXzQ4pTXSCsJfERNj0JfzQxnJ66YU10PxtcMu+hbw0TUu7JhUKMqZRLgL8vOn7/Xbmbf/b7JBSpROEPdy+CHO6wu9joerTlruUSjUwOypN01xMEV83lrzahGYVCzL6pwN8uPwQ8QmJqc9oEp0gMuv4rzC1Kfy9HZ7+BrrNhFz5zI5K0zQXlcfHk+/7PcpLzcsSvPUM/Wfu4GZ0XOozmkAniIyKvwtr34O53cC/KAzeAPX6gYjZkWma5uLc3YT3OlTjs2612H76Kl2mbOFkRKTZYT1AJ4gMyBX9D3z/FGybBI++BAN/h0KVzQ5L07Qspnv9UoS81IhbMXF0mbyFjccizA7pPjpBpNe++dTb9QZcPwM95kKHz8Eze3YWomma49UvU4BlQ5tSIl8u+s/8ix82n3aZ7kx1gkir2Nvw08vw0yAi/crBK1ugakezo9I0LRsomT83i19pwpNVizBuxWFGLTnA3XjzL17rB+XS4sIeWPQiXD8NLUexL/FRHstb0uyoNE3LRny9PZjWpx5f/nqMSX+c4NSVKKb1qUcBXy/TYtJHEA+jFGybDDOegvg70G8FtByJ0s82aJrmAG5uwvA2lfk6KJB9527QadJmwi7eNi8e09bs6iIjYF53WDsaKraGlzdDmaZmR6VpWg7QObAECwY35m58Is9M2cKvhy+ZEodOELacWm/pt+HUBmj/OQTNhdwFzI5K07QcpHapfCwf2ozyhf0YNHsnU9efdPrF65x9DWJiRYi6bHtcwUrQZzEUrencmDRN0wxF8/qwYHBjRizaz4Q1Rzl26TafPlMTH0/nnObO2QkipeQAMGg9ePk6LRRN0zRbfDzd+SYokMpF/Pj8l2OcvhLF9OfrUdjf8bfX61NMKdHJQdM0FyEiDH28ItP61CPs4m06T9rCwfM3Hb7enH0EoWmaloW0rVGUUgUa89KPO+k2bSs96pfityOXuXAjhuL5cjGiTWW61Clht/XpIwhN07QspHrxvCwb2oyieXz4cdtZzt+IQQHnb8QwaskBlu45b7d16QShaZqWxRTy97b5pHVMXAIT14bZbT0OTRAi0lZEwkTkhIiMtDG+v4hEiMhe4zXQalw/ETluvPo5JEDfwukbrmma5iL+uXnH5vALN2Lstg6HXYMQEXdgMvAUEA7sEJHlSqnDySadr5QammzeAsAHQH1AAbuMea/bNcgRx+26OE3TNGcpni8X520kg+L5ctltHY48gmgAnFBKnVJK3QVCgc5pnLcN8KtS6pqRFH4F2jooTk3TtCxnRJvK5Er2PEQuT3dGtLFf1wOOTBAlgHNWn8ONYck9KyL7RWSRiJRK57yapmk5Upc6Jfj0mZqUyJcLAUrky8Wnz9S0611M4qhHt0WkG9BWKTXQ+NwXaGh9OklEAoBIpVSsiAwGeiilHheR4YCPUuojY7r/A2KUUp8nW8cgYBBAkSJF6oWGhjqkLclFRkbi5+fnlHU5k25X1pNd26bb5TytWrXapZSqb2ucI5+DOA+Usvpc0hh2j1LqqtXHGcBnVvO2TDbv+uQrUEpNB6YD1K9fX7Vs2TL5JA6xfv16nLUuZ9Ltynqya9t0u1yDI08x7QAqikhZEfECgoDl1hOISDGrj52AI8b7tUBrEckvIvmB1sYwTdM0zUkcdgShlIoXkaFYNuzuwA9KqUMiMg7YqZRaDgwTkU5APHAN6G/Me01E/oMlyQCMU0pdc1SsmqZp2oMcWmpDKbUKWJVs2Bir96OAUSnM+wPwgyPj0zRN01Kmn6TWNE3TbHLYXUzOJiIRwFknra4gcMVJ63Im3a6sJ7u2TbfLeUorpQrZGpFtEoQzicjOlG4Ly8p0u7Ke7No23S7XoE8xaZqmaTbpBKFpmqbZpBNExkw3OwAH0e3KerJr23S7XIC+BqFpmqbZpI8gNE3TNJt0gtA0TdNs0gniIUTkBxG5LCIHrYYVEJFfjZ7ufjVqRWUpIlJKRP4QkcMickhEXjeGZ4e2+YjIXyKyz2jbWGN4WRHZbvRuON+oD5bliIi7iOwRkRXG5yzfLhE5IyIHjF4ldxrDsvzfIoCI5DO6MjgqIkdEpHFWaptOEA8XzIMdFY0EfldKVQR+Nz5nNfHA20qpakAjYIiIVCN7tC0WeFwpVRsIBNqKSCNgAvBfpVQF4DrwonkhZsrr/FvUErJPu1oppQKtnhHIDn+LAF8Da5RSVYDaWH67rNM2pZR+PeQFlAEOWn0OA4oZ74sBYWbHaIc2LsPSNWy2ahuQG9gNNMTy9KqHMbwxsNbs+DLQnpJYNiiPAysAySbtOgMUTDYsy/8tAnmB0xg3A2XFtukjiPQropT6x3h/EShiZjCZJSJlgDrAdrJJ24zTMHuBy1i6qz0J3FBKxRuTZNUeCr8C3gESjc8BZI92KeAXEdlldAIG2eNvsSwQAcw0TgvOEBFfslDbdILIBGXZBciy9wmLiB+wGHhDKXXLelxWbptSKkEpFYhlj7sBUMXciDJPRDoCl5VSu8yOxQGaKaXqAu2wnO5sYT0yC/8tegB1galKqTpAFMlOJ7l623SCSL9LSR0dGf9eNjmeDBERTyzJYa5SaokxOFu0LYlS6gbwB5ZTL/lEJKm8/QO9G2YBTYFOInIGCMVymulrsn67UEqdN/69DPyEJalnh7/FcCBcKbXd+LwIS8LIMm3TCSL9lgP9jPf9sJy/z1JERIDvgSNKqS+tRmWHthUSkXzG+1xYrq0cwZIouhmTZbm2KaVGKaVKKqXKYOmdcZ1SqjdZvF0i4isi/knvsfQeeZBs8LeolLoInBORysagJ4DDZKG26SepH0JEQrD0jV0QuAR8ACwFFgCPYCkv3l1lsd7uRKQZsAk4wL/ns0djuQ6R1dtWC/gRSy+GbsACpdQ4ESmHZc+7ALAH6KOUijUv0owTkZbAcKVUx6zeLiP+n4yPHsA8pdTHIhJAFv9bBBCRQGAG4AWcAgZg/F2SBdqmE4SmaZpmkz7FpGmaptmkE4SmaZpmk04QmqZpmk06QWiapmk26QShaZqm2aQThGY3IqJE5Aurz8NF5EM7LTtYRLqlPmWm1/OcUXXzj2TDy4hIjFFx9LCITBORDP//EZH1IlLfeL8q6dmNFKbtYhRTTPo8TkSezOi6rZYTLCKnjTbtFZFh6Zy/jIj0ymwcmuvSCUKzp1jgGREpaHYg1qyeNE6LF4GXlFKtbIw7aZTwqAVUA7pkYj33KKXaG099p6SLsb6k6ccopX7LyLpsGKEsVVQDlVLfpHPeMkC6EkRGvyPNHDpBaPYUj6XP3TeTj0h+BCAikca/LUVkg4gsE5FTIjJeRHobfTocEJHyVot5UkR2isgxozZRUmG+iSKyQ0T2i8hgq+VuEpHlWJ5eTR5PT2P5B0VkgjFsDNAM+F5EJqbUSKM43laggoj0F5HlIrIO+N14MvgHI/49ItLZWHYuEQk1jk5+AnJZxXImKamKyPNGO/aJyGwRaQJ0AiYae/nlk75LEWkrIgutltNS/u0norWIbBOR3SKyUCx1t1IlImOM7/KgiEw3nrpHRCqIyG9GXLuN32U80NyI602x9MUx0/he94hIK2Pe5N9RMRHZaMx3UESapyU2zQRml5PVr+zzAiKBPFjKN+cFhgMfGuOCgW7W0xr/tgRuYCl77I2lltBYY9zrwFdW86/BslNTEUudGx9gEPC+MY03sBNLFc2WWIqjlbURZ3Hgb6AQlqd31wFdjHHrgfo25imDUfYdSxnxHViKy/U3YilgjPsEy9PMAPmAY4Av8BbwgzG8FpZkWt/4fAbL0/rVjekLGsMLpPDdBWMpr+FhtMPXGD4V6GMsa6PV8HeBMTbaFIylHPVe41UzaZ3G+NnA08b77UBX472P8R20BFZYTf+2VRurGLH52PiO3gbeM967A/5m/+3ql+2XPoLQ7EpZqsLOAtJzPnuHUuofZSkRcRL4xRh+AMuGOckCpVSiUuo4lrIFVbDU7nleLOW9t2MpgV3RmP4vpdRpG+t7FFivlIpQlqOBuUALG9MlV95YzxZgpVJqtTH8V/VvqYTWwEhjuvVYNpCPGMufA6CU2g/st7H8x4GFSqkrxnQPLb9gxL4GeNo4ddMBS12fRlhOSW0x4ugHlE5hMdanmA4ArcTSQ90BI57qYqmVVEIp9ZOx3jtKqWgby2pm1cajWMpIVLLxHe0ABojl+lRNpdTth7VTM48+H6g5wldYOuqZaTUsHuOUpnFx17prTOvaQYlWnxO5/280eV0YhaXTnNeUUmutR4ilXlFURoJ/iKRrEMlZr0eAZ5VSYcnisXMo94QCQ4FrwE6l1G3jtNCvSqme6VmQiPgAU7Ac2ZwzNuA+dorz3neklNoolpLeHYBgEflSKTXLTuvR7EgfQWh2Z+wpLuD+7i/PAPWM950Azwws+jkRcTPOf5fD0jPXWuAVsZQvR0QqiaUq6MP8BTwmIgVFxB3oCWzIQDy2rAVeszp3X8cYvhHjgq6I1MBymim5dVjaGGBMV8AYfhvwT2F9G7CUkH4JS7IA+BNoKiIVjOX4ikilFOa3lpQMrhjXLLoBGHv44SLSxViet4jkthHXJqC3MU0lLEdO9yVKY1xp4JJS6jsshezqpiE2zQQ6QWiO8gWWc+FJvsOyUd6HpX+GjOzd/41l474aeFkpdQfLBuYwsFtEDgLfksqRsbL05jUSS6nsfcAupZS9Si7/B0vy2y8ih4zPYLk+4CciR4BxwAMd/yilDgEfAxuM7ympFHsoMMK48Fs+2TwJWLofbWf8i1IqAst5/xAR2Q9sIw2dJinLnVTfYSm3vRbLqaAkfYFhxvK2AkWxnCZLMC5cv4nl6MPNOD01H+ivbFeWbQnsE5E9QA8s/VpoLkhXc9U0TdNs0kcQmqZpmk06QWiapmk26QShaZqm2aQThKZpmmaTThCapmmaTTpBaJqmaTbpBKFpmqbZ9P/DYLmzxnCqxgAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "predictive_factors = [8, 16, 32, 64]\n",
        "\n",
        "# Plot HR@10\n",
        "plt.figure()\n",
        "plt.plot(predictive_factors, hr_gmf_final, marker='o', label='GMF')\n",
        "plt.plot(predictive_factors, hr_mlp_final, marker='s', label='MLP')\n",
        "plt.plot(predictive_factors, hr_neuMF_final, marker='^', label='neuMF')\n",
        "plt.xlabel('Number of Predictive Factors')\n",
        "plt.ylabel('HR@10')\n",
        "plt.title('HR@10 vs Predictive Factors')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Plot NDCG@10\n",
        "plt.figure()\n",
        "plt.plot(predictive_factors, ndcg_gmf_final, marker='o', label='GMF')\n",
        "plt.plot(predictive_factors, ndcg_mlp_final, marker='s', label='MLP')\n",
        "plt.plot(predictive_factors, ndcg_neuMF_final, marker='^', label='neuMF')\n",
        "plt.xlabel('Number of Predictive Factors')\n",
        "plt.ylabel('NDCG@10')\n",
        "plt.title('NDCG@10 vs Predictive Factors')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbWNuFk7bvSJ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DWnUKhGcX24"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73RGBr1wd5_L"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.9 64-bit ('base': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9-final"
    },
    "vscode": {
      "interpreter": {
        "hash": "0355315a2ab493fe43adfbd0e3e2d1269f7b92070986f6468c200dc91ca7ae56"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
